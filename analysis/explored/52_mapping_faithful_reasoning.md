# Paper Analysis: Mapping Faithful Reasoning in Language Models

## Metadata
- **arXiv ID**: 2510.22362
- **Title**: Mapping Faithful Reasoning in Language Models
- **Authors**: Jiazheng Li et al. (King's College London, Spotify, Oxford)
- **Date**: October 2025
- **Venue**: NeurIPS '24 proceedings

---

## Core Claims

1. **CoT traces are not always faithful** — may be post-hoc rationalisations rather than genuine computation
2. **"Concept Walk" framework** — traces internal activation evolution during reasoning
3. **Easy vs Hard case distinction** — determines when CoT is decorative vs computational
4. **Perturbation persistence indicates faithfulness** — sustained shifts = faithful, transient = decorative

---

## Methodology

### Three-Stage Pipeline

**Stage 1: Filtering for CoT-as-computation**
- Perturbation-sensitivity filtering
- Retain examples where perturbing CoT significantly changes output
- Filters out "easy" cases (rationalisation), retains "hard" cases (computation)

**Stage 2: Computing the Concept Vector (Safety Example)**
- Difference of Means approach between safe/unsafe datasets
- Safety direction: v = μ_unsafe - μ_safe
- Selection criteria: bypass score, induce score, KL divergence, layer exclusion

**Stage 3: The Concept Walk**
- Run model in thinking mode
- At each CoT step, extract residual stream activations
- Compute cosine similarity with concept vector
- Plot trajectory over reasoning steps

### Perturbation Procedure
1. Generate baseline CoT
2. Inject controlled mistake at middle step
3. Regenerate rest of CoT
4. Compare baseline vs perturbed internal trajectories
5. Sustained shift = faithful; transient shift = decorative

### Model & Data
- **Model**: Qwen 3-4B (36 layers, 4B parameters)
- **Domain**: Music AI assistant (playlist requests)
- **Dataset**: 7,730 harm/hate prompt pairs (generated by Mistral-7B)

---

## Key Evidence

### Hard vs Easy Case Divergence

**Hard cases (CoT-as-computation):**
- Perturbing reasoning → **sustained and structured changes** in internal activation
- Spikes persist over multiple reasoning steps
- Model integrates modified reasoning into decision

**Easy cases (CoT-as-rationalisation):**
- **Much smaller, transient perturbation effects**
- Model **rapidly reverts** to original trajectory
- "Self-correction" behavior — reasoning is decorative

### Violative vs Non-violative Pattern
- Violative cases trigger **much higher initial safety activation**
- Confirms safety vector captures model's internal recognition
- Internal state differs from surface text

### Qualitative Examples

**Hard case** ("classic rock tunes"):
- Injected flaw at step 8 → sustained shift in safety activation
- Model's decision ultimately changes

**Easy case** ("bar crawl playlist"):
- Only transient shifts after flaw injection
- Model quickly reverts to original trajectory

---

## Critical Assessment

### What This Paper Shows

1. **CoT faithfulness varies by task difficulty** — easy tasks have decorative CoT
2. **Internal activations reveal what text cannot** — model may "know" answer before reasoning
3. **Perturbation persistence = faithfulness indicator** — mechanistic tool for assessment
4. **Activation-space analysis complements surface evaluation** — goes beyond text

### Key Insight

The paper provides a **mechanistic method** to distinguish:
- **Decorative reasoning**: Model knows answer, writes plausible explanation
- **Computational reasoning**: Model actually uses reasoning to reach answer

### Relevance to Thesis

**SUPPORTS thesis with important nuance**

**Supports thesis:**
- Confirms CoT is often post-hoc rationalisation (easy cases)
- Model can "know" answer before reasoning — reasoning is window dressing
- Surface text doesn't reflect internal computation

**Nuances thesis:**
- "Hard" cases show faithful reasoning IS possible
- Method distinguishes when reasoning is genuine vs decorative
- Not all CoT is unfaithful — depends on task difficulty

---

## Relationship to Other Papers

### Supports
- **Measuring Faithfulness (2307.13702)**: Provides mechanistic evidence for unfaithfulness
- **FRIT (2509.13334)**: Both find easy/hard case distinction
- **Reasoning Models Don't Say (2505.05410)**: Both show CoT can be decorative

### Extends
- **Perturbation studies**: Adds activation-space analysis to surface evaluation
- **Mechanistic interpretability**: Novel "Concept Walk" framework

### Provides Method For
- **Detecting unfaithful reasoning**: Concept Walk + perturbation analysis
- **AI safety monitoring**: Distinguishing computational vs decorative reasoning

---

## REBUTTALS TO THIS PAPER

### Search for Direct Rebuttals
- No direct rebuttals found (recent paper)

### Potential Counter-Arguments

1. **Mode mismatch** — Safety vector from non-thinking mode applied to thinking mode
2. **Surface-template artifacts** — Direction could encode refusal patterns
3. **Limited model scope** — Only Qwen 3-4B tested
4. **Small sample sizes** — Hard cases have small cohorts

### Limitations (Authors Acknowledge)
- Mode mismatch (safety direction from different mode)
- Surface-template artifacts possible
- Incomplete faithfulness guarantee
- Hidden reasoning pathways not captured
- Single model (Qwen 3-4B only)
- Small sample sizes for hard violative cases

---

## Key Quotes

> "Chain-of-thought (CoT) traces promise transparency for reasoning language models, but prior work shows they are not always faithful reflections of internal computation."

> "In 'easy' cases, perturbed CoTs are quickly ignored, indicating decorative reasoning, whereas in 'hard' cases, perturbations induce sustained shifts in internal activations, consistent with faithful reasoning."

> "Rather than reflecting genuine reasoning pathways, these traces may function primarily as post-hoc rationalisations."

---

## Relevance to Thesis

**BALANCED — Provides mechanistic evidence for both faithful and unfaithful reasoning**

This paper shows:
1. ✓ CoT is often decorative (easy cases) — supports thesis
2. ✓ Model can "know" answer before reasoning — post-hoc rationalisation
3. ~ But: Hard cases show faithful reasoning IS possible
4. ~ Method distinguishes genuine from decorative reasoning

**Key insight for thesis**: The paper provides a **tool** to identify when reasoning is genuine vs decorative. Easy tasks → decorative (supports thesis). Hard tasks → computational (nuances thesis). The distinction is important: our thesis should acknowledge that SOME reasoning may be genuine, particularly for harder tasks.

---

## Status
- [x] Read complete (HTML version via Task agent)
- [x] Core claims extracted
- [x] Methodology documented
- [x] Key evidence with numbers
- [x] Cross-references identified
- [x] **Rebuttals checked**
- [x] **Paper graph updated**

---

## Verdict: BALANCED (provides mechanistic method; confirms easy cases are decorative; shows hard cases may be faithful)
