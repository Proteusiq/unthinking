# Papers from LLM Reasoning Failures Survey

Source: [arXiv:2602.06176](https://arxiv.org/abs/2602.06176) - Song, Han, Goodman (TMLR 2026)
GitHub: https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures

**Last updated**: 2026-02-11
**Tracking Issues**: #48-55

---

## Status Legend

- âœ… DONE - Already analyzed in our corpus
- ðŸ“‹ TO-READ - In toread.md (Issues #48-54)
- ðŸ”— LINKED - Link found, not yet triaged

---

## Surveys

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| âœ… DONE | [2602.06176](https://arxiv.org/abs/2602.06176) | Large Language Model Reasoning Failures | TMLR 2026 |
| ðŸ”— LINKED | [2503.13657](https://arxiv.org/abs/2503.13657) | Why Do Multi-Agent LLM Systems Fail? | NeurIPS 2025 |

---

## Informal Reasoning - Individual Cognitive Skills and Biases

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| ðŸ“‹ TO-READ | [2305.03731](https://arxiv.org/abs/2305.03731) | Working memory capacity of ChatGPT | AAAI 2024 |
| ðŸ”— LINKED | [EMNLP](https://aclanthology.org/2024.emnlp-main.938/) | Working memory identifies reasoning limits in language models | EMNLP 2024 |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=dXjQgm9kAr) | Self-Attention Limits Working Memory Capacity of Transformer-Based Models | NeurIPS 2024 Workshop |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=YUHksmL8aw) | Unable to Forget: Proactive Interference Reveals Working Memory Limits | ICML 2025 Workshop |
| ðŸ“‹ TO-READ | [2505.10571](https://arxiv.org/abs/2505.10571) | LLMs Do Not Have Human-Like Working Memory | arXiv |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=II0NVPLBcI) | Working memory attack on LLMs | ICLR 2025 Workshop |
| âœ… DONE | [2409.15454](https://arxiv.org/abs/2409.15454) | In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors | EMNLP 2024 |
| ðŸ”— LINKED | [bioRxiv](https://www.biorxiv.org/content/10.1101/2025.01.22.634394v1) | Deficient Executive Control in Transformer Attention | bioRxiv |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=58yThpzlth) | Cognitive flexibility of large language models | ICML 2024 Workshop |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=E8m8oySvPJ) | LLMs and the Abstraction and Reasoning Corpus | TMLR 2024 |
| ðŸ“‹ TO-READ | [2305.19555](https://arxiv.org/abs/2305.19555) | Large language models are not strong abstract reasoners | arXiv |
| ðŸ“‹ TO-READ | [2410.11756](https://arxiv.org/abs/2410.11756) | Evidence of Cognitive Deficits: Clock Drawing Test Analysis | arXiv |
| ðŸ“‹ TO-READ | [2502.05092](https://arxiv.org/abs/2502.05092) | Lost in Time: Clock and Calendar Understanding Challenges | arXiv |
| âœ… DONE | [2207.07051](https://arxiv.org/abs/2207.07051) | Language models, like humans, show content effects on reasoning | PNAS Nexus 2024 |
| ðŸ”— LINKED | [IEEE](https://ieeexplore.ieee.org/document/10897252) | Confirmation and Specificity Biases in Large Language Models | IEEE Intelligent Systems |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2025.findings-acl.195/) | Unveiling Confirmation Bias in Chain-of-Thought Reasoning | ACL 2025 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2025.acl-long.195/) | Conformity in Large Language Models | ACL 2025 |
| ðŸ“‹ TO-READ | [2412.04629](https://arxiv.org/abs/2412.04629) | Argumentative Experience: Reducing Confirmation Bias | arXiv |
| ðŸ“‹ TO-READ | [2410.15413](https://arxiv.org/abs/2410.15413) | A Comprehensive Evaluation of Cognitive Biases in LLMs | arXiv |
| ðŸ“‹ TO-READ | [2403.00811](https://arxiv.org/abs/2403.00811) | Cognitive bias in high-stakes decision-making with LLMs | EMNLP 2024 |
| âœ… DONE | [2408.00137](https://arxiv.org/abs/2408.00137) | Correcting negative bias through negative attention score alignment | arXiv |
| âœ… DONE | [2202.12299](https://arxiv.org/abs/2202.12299) | Capturing failures of large language models via human cognitive biases | NeurIPS 2022 |
| ðŸ“‹ TO-READ | [2305.04400](https://arxiv.org/abs/2305.04400) | Do large language models show decision heuristics similar to humans? | J Exp Psych 2024 |
| ðŸ”— LINKED | [ScienceDirect](https://www.sciencedirect.com/science/article/pii/S2214635024000868) | Human bias in AI models? Anchoring effects and mitigation strategies | J Behavioral Finance |
| ðŸ”— LINKED | [IEEE](https://ieeexplore.ieee.org/document/10962248) | An Anchoring Effect in Large Language Models | IEEE Intelligent Systems 2025 |
| ðŸ“‹ TO-READ | [2505.15392](https://arxiv.org/abs/2505.15392) | An Empirical Study of the Anchoring Effect in LLMs | arXiv |
| ðŸ“‹ TO-READ | [2504.09946](https://arxiv.org/abs/2504.09946) | Assessing Judging Bias in Large Reasoning Models | arXiv |
| ðŸ“‹ TO-READ | [2502.17091](https://arxiv.org/abs/2502.17091) | WildFrame: Comparing Framing in Humans and LLMs | arXiv |
| ðŸ“‹ TO-READ | [2503.04840](https://arxiv.org/abs/2503.04840) | Framing the Game: How Context Shapes LLM Decision-Making | arXiv |
| ðŸ”— LINKED | [arXiv](https://arxiv.org/abs/2503.04840) | Investigating bias in LLM-based bias detection | COLING 2025 |
| ðŸ“‹ TO-READ | [2506.03923](https://arxiv.org/abs/2506.03923) | More or Less Wrong: Directional Bias in LLM Comparative Reasoning | arXiv |
| ðŸ“‹ TO-READ | [2310.10076](https://arxiv.org/abs/2310.10076) | Verbosity bias in preference labeling by large language models | arXiv |
| ðŸ“‹ TO-READ | [2505.22910](https://arxiv.org/abs/2505.22910) | Talent or Luck? Evaluating Attribution Bias in LLMs | arXiv |
| ðŸ“‹ TO-READ | [2406.01285](https://arxiv.org/abs/2406.01285) | Large language models as recommender systems: Popularity bias | arXiv |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=YiIdHqqoCd) | Beyond Utility: Evaluating LLM as Recommender | WWW 2025 |
| ðŸ“‹ TO-READ | [2507.22887](https://arxiv.org/abs/2507.22887) | Where to show Demos in Your Prompt: Positional Bias | arXiv |
| âœ… DONE | [2308.11483](https://arxiv.org/abs/2308.11483) | Large language models sensitivity to the order of options | NAACL 2024 |
| ðŸ”— LINKED | [IJAIRD](https://iaeme.com/Home/article_id/IJAIRD_02_02_010) | Mitigating order sensitivity in large language models | IJAIRD 2024 |
| ðŸ“‹ TO-READ | [2502.04134](https://arxiv.org/abs/2502.04134) | The Order Effect: Investigating Prompt Sensitivity to Input Order | KDD 2025 Workshop |
| ðŸ“‹ TO-READ | [2412.06593](https://arxiv.org/abs/2412.06593) | Anchoring Bias in Large Language Models: An Experimental Study | arXiv |
| ðŸ”— LINKED | [ACM](https://dl.acm.org/doi/10.1145/3613905.3650818) | Believing Anthropomorphism: Trust in Large Language Models | CHI 2024 |
| âœ… DONE | [2309.17012](https://arxiv.org/abs/2309.17012) | Benchmarking cognitive biases in large language models as evaluators | ACL 2024 |
| âœ… DONE | [2302.00093](https://arxiv.org/abs/2302.00093) | Large Language Models Can Be Easily Distracted by Irrelevant Context | ICML 2023 |
| âœ… DONE | [2308.00225](https://arxiv.org/abs/2308.00225) | Instructed to bias: emergent cognitive bias in instruction-tuned LMs | TACL 2024 |
| âœ… DONE | [2303.13988](https://arxiv.org/abs/2303.13988) | Machine psychology: Investigating emergent capabilities | arXiv |
| ðŸ”— LINKED | [PDF](https://neurosymbolic-ai-journal.com/system/files/nai-paper-819.pdf) | Cognitive LLMs: Human-Like AI for Manufacturing Decision-making | Neurosymbolic AI 2024 |

---

## Informal Reasoning - Implicit Social Reasoning (Theory of Mind)

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.conll-1.25/) | Theory of mind in large language models vs children aged 7-10 | CoNLL 2023 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.emnlp-main.890.pdf) | FANToM: A benchmark for stress-testing machine theory of mind | EMNLP 2023 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2022.emnlp-main.248/) | Neural theory-of-mind? On the limits of social intelligence in large LMs | EMNLP 2022 |
| ðŸ“‹ TO-READ | [2406.14737](https://arxiv.org/abs/2406.14737) | Dissecting the Ullman Variations with a SCALPEL | arXiv |
| ðŸ“‹ TO-READ | [2302.08399](https://arxiv.org/abs/2302.08399) | Large language models fail on trivial alterations to theory-of-mind tasks | arXiv |
| ðŸ”— LINKED | [PNAS](https://www.pnas.org/doi/10.1073/pnas.2405460121) | Evaluating large language models in theory of mind tasks | PNAS 2024 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.eacl-long.138/) | Clever hans or neural theory of mind? Stress testing social reasoning | EACL 2024 |
| ðŸ“‹ TO-READ | [2410.13648](https://arxiv.org/abs/2410.13648) | SimpleToM: Exposing the Gap between Explicit ToM Inference | arXiv |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.findings-emnlp.717/) | Hi-ToM: A benchmark for evaluating higher-order theory of mind | EMNLP 2023 |
| ðŸ“‹ TO-READ | [2310.03051](https://arxiv.org/abs/2310.03051) | How FaR Are Large Language Models From Agents with Theory-of-Mind? | arXiv |
| ðŸ”— LINKED | [Nature](https://www.nature.com/articles/s41562-024-01882-z) | Testing theory of mind in large language models and humans | Nature Human Behaviour 2024 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.acl-long.780.pdf) | Minding Language Models' (Lack of) Theory of Mind: Belief Tracker | ACL 2023 |
| ðŸ”— LINKED | [PubMed](https://pubmed.ncbi.nlm.nih.gov/40333375/) | Artificial Intelligence and the Illusion of Understanding: Systematic Review | Cyberpsych 2025 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2025.acl-long.1171/) | Towards Dynamic Theory of Mind: Evaluating LLM Adaptation | ACL 2025 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.acl-long.326/) | EmoBench: Evaluating the Emotional Intelligence of Large Language Models | ACL 2024 |
| ðŸ“‹ TO-READ | [2502.04424](https://arxiv.org/abs/2502.04424) | EmoBench-M: Benchmarking Emotional Intelligence for Multimodal LLMs | arXiv |
| ðŸ”— LINKED | [ACM](https://dl.acm.org/doi/10.1145/3627673.3679832) | Can LLMs Reason Like Humans? Assessing ToM for Open-Ended Questions | CIKM 2024 |
| ðŸ”— LINKED | [PubMed](https://pubmed.ncbi.nlm.nih.gov/39552777/) | The Emotional Intelligence of the GPT-4 Large Language Model | Psychol Russ 2024 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.wassa-1.19/) | Multilingual Language Models are not Multicultural: Emotion | ACL 2023 WASSA |
| ðŸ“‹ TO-READ | [2406.04428](https://arxiv.org/abs/2406.04428) | MoralBench: Moral Evaluation of LLMs | arXiv |
| ðŸ”— LINKED | [ACM](https://dl.acm.org/doi/10.5555/3716662.3716716) | "As an AI Language Model," Norm Inconsistency in LLM Decision-Making | AIES 2024 |
| ðŸ“‹ TO-READ | [2402.01719](https://arxiv.org/abs/2402.01719) | Measuring Moral Inconsistencies in Large Language Models | arXiv |
| ðŸ“‹ TO-READ | [2309.13356](https://arxiv.org/abs/2309.13356) | Probing the moral development of large language models | arXiv |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.lrec-main.560/) | Ethical reasoning and moral value alignment depend on language | ACL 2024 LREC |
| ðŸ“‹ TO-READ | [2404.02934](https://arxiv.org/abs/2404.02934) | GreedLlama: Performance of financial value-aligned LLMs | arXiv |
| ðŸ“‹ TO-READ | [2502.20490](https://arxiv.org/abs/2502.20490) | EgoNormia: Benchmarking Physical Social Norm Understanding | arXiv |
| ðŸ“‹ TO-READ | [2410.07304](https://arxiv.org/abs/2410.07304) | The Moral Turing Test: Evaluating Human-LLM Alignment | arXiv |
| ðŸ”— LINKED | [Royal Society](https://royalsocietypublishing.org/doi/10.1098/rsos.231393) | The moral machine experiment on large language models | Royal Society |
| ðŸ”— LINKED | [Nature](https://www.nature.com/articles/s42256-024-00969-6) | Investigating machine moral judgement through the Delphi experiment | Nature Machine Intelligence 2025 |

---

## Informal Reasoning - Explicit Social Reasoning

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.emnlp-main.13/) | Theory of mind for multi-agent collaboration via large language models | EMNLP 2023 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2025.acl-long.1496.pdf) | SocialEval: Evaluating social intelligence of large language models | ACL 2025 |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=otW0TJOUYF) | Hypothetical minds: Scaffolding theory of mind for multi-agent tasks | ICLR 2025 |
| ðŸ”— LINKED | [IJCAI](https://www.ijcai.org/proceedings/2024/890) | Large language model based multi-agents: A survey | IJCAI 2024 |
| ðŸ“‹ TO-READ | [2402.03578](https://arxiv.org/abs/2402.03578) | LLM multi-agent systems: Challenges and open problems | arXiv |
| ðŸ“‹ TO-READ | [2404.16698](https://arxiv.org/abs/2404.16698) | Cooperate or collapse: Emergence of sustainable cooperation | NeurIPS 2024 |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=EnXJfQqy0K) | Building cooperative embodied agents modularly with LLMs | ICLR 2024 |
| ðŸ“‹ TO-READ | [2310.03903](https://arxiv.org/abs/2310.03903) | LLM-Coordination: Evaluating Multi-agent Coordination Abilities | arXiv |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=wM521FqPvI) | Why Do Multiagent Systems Fail? | ICLR 2025 Workshop |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=vgDdq5Tej8) | On the resilience of multi-agent systems with malicious agents | CoRR 2024 |
| ðŸ“‹ TO-READ | [2503.11926](https://arxiv.org/abs/2503.11926) | Monitoring Reasoning Models for Misbehavior | arXiv |
| ðŸ“‹ TO-READ | [2311.08562](https://arxiv.org/abs/2311.08562) | Magic: Investigation of LLM multi-agent cognition | EMNLP 2024 |

---

## Formal Reasoning - Logic in Natural Languages

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| âœ… DONE | [2309.12288](https://arxiv.org/abs/2309.12288) | The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A" | ICLR 2024 |
| ðŸ“‹ TO-READ | [2312.03633](https://arxiv.org/abs/2312.03633) | Exploring the Reversal Curse in BERT and GPT-Based LLMs | Patterns 2024 |
| ðŸ“‹ TO-READ | [2403.13799](https://arxiv.org/abs/2403.13799) | Reverse Training to Nurse the Reversal Curse | COLM 2024 |
| ðŸ“‹ TO-READ | [2402.01453](https://arxiv.org/abs/2402.01453) | The Queen of England is not England's Queen | EACL 2024 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.findings-acl.811/) | Exploring Reversal Mathematical Reasoning Ability for LLMs | ACL 2024 |
| ðŸ“‹ TO-READ | [2403.00758](https://arxiv.org/abs/2403.00758) | Mitigating Reversal Curse via Semantic-aware Permutation Training | ACL 2024 |
| ðŸ“‹ TO-READ | [2311.07468](https://arxiv.org/abs/2311.07468) | An Analysis and Mitigation of the Reversal Curse | EMNLP 2024 |
| ðŸ“‹ TO-READ | [2310.10322](https://arxiv.org/abs/2310.10322) | Untying the Reversal Curse via Bidirectional Language Model Editing | arXiv |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.emnlp-main.428/) | Rethinking the Reversal Curse of LLMs: Human Knowledge Reversal | EMNLP 2024 |
| ðŸ“‹ TO-READ | [2410.18808](https://arxiv.org/abs/2410.18808) | Delving into the Reversal Curse: How Far Can LLMs Generalize? | NeurIPS 2024 |
| ðŸ“‹ TO-READ | [2405.04669](https://arxiv.org/abs/2405.04669) | Towards a Theoretical Understanding of the 'Reversal Curse' | NeurIPS 2024 |
| ðŸ“‹ TO-READ | [2411.16353](https://arxiv.org/abs/2411.16353) | The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C | arXiv |
| ðŸ“‹ TO-READ | [2502.13913](https://arxiv.org/abs/2502.13913) | How Do LLMs Perform Two-Hop Reasoning in Context? | arXiv |
| ðŸ“‹ TO-READ | [2403.02615](https://arxiv.org/abs/2403.02615) | Exploring Limitations in Compositional Relation Reasoning | COLM 2024 |
| âœ… DONE | [2305.18654](https://arxiv.org/abs/2305.18654) | Faith and Fate: Limits of Transformers on Compositionality | NeurIPS 2023 |
| ðŸ“‹ TO-READ | [2405.06680](https://arxiv.org/abs/2405.06680) | Exploring the Compositional Deficiency in Mathematical Reasoning | EMNLP 2024 |
| ðŸ”— LINKED | [1905.10044](https://arxiv.org/abs/1905.10044) | BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions | NAACL 2019 |
| ðŸ“‹ TO-READ | [2401.00757](https://arxiv.org/abs/2401.00757) | LogicAsker: Evaluating Logical Reasoning Ability of LLMs | EMNLP 2024 |
| ðŸ“‹ TO-READ | [2306.12567](https://arxiv.org/abs/2306.12567) | Evaluating LLMs with NeuBAROCO: Syllogistic Reasoning | NALOMA IV |
| ðŸ“‹ TO-READ | [2310.05163](https://arxiv.org/abs/2310.05163) | An Investigation of LLMs' Inefficacy in Understanding Converse Relations | EMNLP 2023 |
| ðŸ“‹ TO-READ | [2402.10735](https://arxiv.org/abs/2402.10735) | Assessing the Reasoning Abilities of ChatGPT in Claim Verification | arXiv |
| ðŸ“‹ TO-READ | [2406.12158](https://arxiv.org/abs/2406.12158) | LLMs Are Prone to Fallacies in Causal Inference | EMNLP 2024 |
| ðŸ“‹ TO-READ | [2410.16502](https://arxiv.org/abs/2410.16502) | Rulebreakers Challenge: Blind Spot in Formal Logic | arXiv |
| ðŸ“‹ TO-READ | [2402.11051](https://arxiv.org/abs/2402.11051) | LLMs Fall Short: Complex Relationships in Detective Narratives | ACL 2024 |
| ðŸ“‹ TO-READ | [2410.01748](https://arxiv.org/abs/2410.01748) | Not All LLM Reasoners Are Created Equal | arXiv |
| ðŸ“‹ TO-READ | [2407.15720](https://arxiv.org/abs/2407.15720) | Do Large Language Models Have Compositional Ability? | COLM 2024 |
| ðŸ“‹ TO-READ | [2402.14328](https://arxiv.org/abs/2402.14328) | Understanding and Patching Compositional Reasoning in LLMs | ACL 2024 |
| ðŸ“‹ TO-READ | [2411.16679](https://arxiv.org/abs/2411.16679) | Do LLMs Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts? | ACL 2024 |
| ðŸ“‹ TO-READ | [2402.01805](https://arxiv.org/abs/2402.01805) | Enhancing Logical Reasoning through Graph-based Synthetic Data | arXiv |
| ðŸ“‹ TO-READ | [2408.15778](https://arxiv.org/abs/2408.15778) | LogicGame: Benchmarking Rule-Based Reasoning Abilities | ACL 2024 |
| ðŸ“‹ TO-READ | [2402.11442](https://arxiv.org/abs/2402.11442) | Can LLMs Reason with Rules? Logic Scaffolding | ACL 2024 |
| ðŸ“‹ TO-READ | [2408.08978](https://arxiv.org/abs/2408.08978) | See What LLMs Cannot Answer: Self-Challenge Framework | COLM 2024 |

---

## Formal Reasoning - Logic in Benchmarks

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| ðŸ“‹ TO-READ | [2309.03882](https://arxiv.org/abs/2309.03882) | Large Language Models Are Not Robust Multiple Choice Selectors | ICLR 2024 |
| ðŸ“‹ TO-READ | [2402.01781](https://arxiv.org/abs/2402.01781) | When benchmarks are targets: Sensitivity of leaderboards | ACL 2024 |
| ðŸ“‹ TO-READ | [2406.19470](https://arxiv.org/abs/2406.19470) | Changing Answer Order Can Decrease MMLU Accuracy | arXiv |
| ðŸ“‹ TO-READ | [2402.08939](https://arxiv.org/abs/2402.08939) | Premise Order Matters in Reasoning with Large Language Models | ICML 2024 |
| ðŸ“‹ TO-READ | [2410.23884](https://arxiv.org/abs/2410.23884) | Failure Modes of LLMs for Causal Reasoning on Narratives | arXiv |
| âœ… DONE | [2406.11050](https://arxiv.org/abs/2406.11050) | A Peek into Token Bias: LLMs Are Not Yet Genuine Reasoners | EMNLP 2024 |
| âœ… DONE | [2410.05229](https://arxiv.org/abs/2410.05229) | GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning | arXiv |
| âœ… DONE | [2307.02477](https://arxiv.org/abs/2307.02477) | Reasoning or Reciting? Counterfactual Tasks | NAACL 2024 |
| ðŸ“‹ TO-READ | [2402.19255](https://arxiv.org/abs/2402.19255) | GSM-Plus: Evaluating the Robustness of LLMs as Math Solvers | ACL 2024 |
| ðŸ“‹ TO-READ | [2103.07191](https://arxiv.org/abs/2103.07191) | Are NLP Models really able to Solve Simple Math Word Problems? | NAACL 2021 |
| ðŸ“‹ TO-READ | [2212.10264](https://arxiv.org/abs/2212.10264) | ReCode: Robustness Evaluation of Code Generation Models | ACL 2023 |
| ðŸ“‹ TO-READ | [2402.05980](https://arxiv.org/abs/2402.05980) | Do Large Code Models Understand Programming Concepts? | ICML 2024 |
| ðŸ“‹ TO-READ | [2403.19114](https://arxiv.org/abs/2403.19114) | EvoEval: Evolving Coding Benchmarks via LLM | COLM 2024 |
| ðŸ“‹ TO-READ | [2306.03438](https://arxiv.org/abs/2306.03438) | LLMs of Code Fail at Completing Code with Potential Bugs | NeurIPS 2023 |
| ðŸ“‹ TO-READ | [2305.15507](https://arxiv.org/abs/2305.15507) | The Larger They Are, the Harder They Fail: Identifier Swaps | ACL 2023 |
| ðŸ“‹ TO-READ | [2404.01535](https://arxiv.org/abs/2404.01535) | Syntactic Robustness for LLM-based Code Generation | arXiv |
| ðŸ“‹ TO-READ | [2406.11020](https://arxiv.org/abs/2406.11020) | RUPBench: Benchmarking Reasoning Under Perturbations | arXiv |
| ðŸ“‹ TO-READ | [2401.09395](https://arxiv.org/abs/2401.09395) | Evaluating LLMs' Math and Coding via Ontology-guided Interventions | arXiv |
| ðŸ“‹ TO-READ | [2310.01991](https://arxiv.org/abs/2310.01991) | Fill in the Blank: Backward Reasoning in Math Word Problems | arXiv |
| ðŸ“‹ TO-READ | [2502.06453](https://arxiv.org/abs/2502.06453) | MATH-Perturb: Benchmarking LLMs' Math Reasoning against Perturbations | arXiv |
| ðŸ“‹ TO-READ | [2505.20296](https://arxiv.org/abs/2505.20296) | Reasoning LLMs are Wandering Solution Explorers | arXiv |
| âœ… DONE | [2506.06941](https://arxiv.org/abs/2506.06941) | The Illusion of Thinking | arXiv |
| âœ… DONE | [2506.09250](https://arxiv.org/abs/2506.09250) | Comment on The Illusion of Thinking | arXiv |
| âœ… DONE | [2506.18880](https://arxiv.org/abs/2506.18880) | OMEGA: Can LLMs Reason Outside the Box in Math? | arXiv |
| ðŸ“‹ TO-READ | [2507.13337](https://arxiv.org/abs/2507.13337) | FormulaOne: Measuring the Depth of Algorithmic Reasoning | arXiv |

---

## Formal Reasoning - Arithmetic and Mathematics

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| ðŸ“‹ TO-READ | [2412.18626](https://arxiv.org/abs/2412.18626) | Why Do Large Language Models (LLMs) Struggle to Count Letters? | arXiv |
| âœ… DONE | [2507.07313](https://arxiv.org/abs/2507.07313) | Frontier LLMs Still Struggle with Simple Reasoning Tasks | arXiv |
| ðŸ“‹ TO-READ | [2410.19730](https://arxiv.org/abs/2410.19730) | Counting Ability of LLMs and Impact of Tokenization | arXiv |
| ðŸ“‹ TO-READ | [2405.20131](https://arxiv.org/abs/2405.20131) | Language Models Need Inductive Biases to Count Inductively | ICLR 2025 |
| ðŸ“‹ TO-READ | [2410.14166](https://arxiv.org/abs/2410.14166) | LLM The Genius Paradox: Word-based Counting Problems | arXiv |
| ðŸ“‹ TO-READ | [2407.15160](https://arxiv.org/abs/2407.15160) | When Can Transformers Count to n? | arXiv |
| ðŸ“‹ TO-READ | [2405.11357](https://arxiv.org/abs/2405.11357) | Large Language Models Lack Understanding of Character Composition | arXiv |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.findings-emnlp.691/) | LLMs Can Not Perform Well at Char/Word Level Manipulation | EMNLP 2024 |
| ðŸ”— LINKED | [MDPI](https://www.mdpi.com/2076-3417/14/2/744) | Can Neural Networks Do Arithmetic? A Survey | Applied Sciences 2024 |
| ðŸ“‹ TO-READ | [2403.19346](https://arxiv.org/abs/2403.19346) | Large Language Models Are Unconscious of Unreasonability in Math | arXiv |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=WrBqgoseGL) | Putnam-AXIOM: A Functional and Static Benchmark for Higher Level Math | OpenReview |
| ðŸ“‹ TO-READ | [2406.17681](https://arxiv.org/abs/2406.17681) | VarBench: Robust Language Model Benchmarking | EMNLP 2024 |
| âœ… DONE | [2406.02061](https://arxiv.org/abs/2406.02061) | Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown | arXiv |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.findings-acl.524/) | Rationales for Answers to Simple Math Word Problems Confuse LLMs | ACL 2024 |
| ðŸ“‹ TO-READ | [2406.09072](https://arxiv.org/abs/2406.09072) | Living in the Moment: Can LLMs Grasp Co-Temporal Reasoning? | ACL 2024 |
| ðŸ“‹ TO-READ | [2501.02825](https://arxiv.org/abs/2501.02825) | Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs | arXiv |
| ðŸ“‹ TO-READ | [2410.18921](https://arxiv.org/abs/2410.18921) | From Blind Solvers to Logical Thinkers | arXiv |
| ðŸ“‹ TO-READ | [2304.02015](https://arxiv.org/abs/2304.02015) | How well do Large Language Models perform in Arithmetic tasks? | arXiv |
| ðŸ“‹ TO-READ | [2410.13857](https://arxiv.org/abs/2410.13857) | How Numerical Precision Affects Mathematical Reasoning | arXiv |
| ðŸ“‹ TO-READ | [2406.02356](https://arxiv.org/abs/2406.02356) | Language Models Do Hard Arithmetic Easily and Easy Arithmetic Hardly | ACL 2024 |
| ðŸ“‹ TO-READ | [2410.15580](https://arxiv.org/abs/2410.15580) | Language Models are Symbolic Learners in Arithmetic | arXiv |
| ðŸ“‹ TO-READ | [2406.06576](https://arxiv.org/abs/2406.06576) | OccamLLM: Fast and Exact Language Model Arithmetic | NeurIPS 2024 |
| ðŸ“‹ TO-READ | [2309.03241](https://arxiv.org/abs/2309.03241) | GPT Can Solve Mathematical Problems Without a Calculator | arXiv |
| ðŸ“‹ TO-READ | [2406.05055](https://arxiv.org/abs/2406.05055) | Robustness Assessment with Missing and Contradictory Conditions | arXiv |
| ðŸ“‹ TO-READ | [2403.05845](https://arxiv.org/abs/2403.05845) | Reverse That Number! Decoding Order Matters in Arithmetic | arXiv |
| ðŸ“‹ TO-READ | [2402.03822](https://arxiv.org/abs/2402.03822) | RevOrder: A Novel Method for Enhanced Arithmetic in LMs | arXiv |
| âœ… DONE | [2410.21272](https://arxiv.org/abs/2410.21272) | Arithmetic Without Algorithms: Language Models Solve Math with Heuristics | ICLR 2025 |
| ðŸ“‹ TO-READ | [2306.16636](https://arxiv.org/abs/2306.16636) | CMATH: Can Your Language Model Pass Chinese Elementary School Math? | arXiv |
| ðŸ“‹ TO-READ | [2502.11574](https://arxiv.org/abs/2502.11574) | Large Language Models and Mathematical Reasoning Failures | arXiv |
| ðŸ“‹ TO-READ | [2410.09988](https://arxiv.org/abs/2410.09988) | HARDMath: A Benchmark Dataset for Challenging Applied Mathematics | arXiv |
| ðŸ“‹ TO-READ | [2502.01612](https://arxiv.org/abs/2502.01612) | Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization | arXiv |

---

## Embodied Reasoning - 1D Text-Based Physical Reasoning

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2021.findings-acl.404/) | Prost: Physical reasoning about objects through space and time | ACL 2021 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.conll-1.27.pdf) | TEXT2AFFORD: Probing Object Affordance Prediction from Text | CoNLL 2024 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.lrec-main.74/) | A Multi-layered Approach to Physical Commonsense Understanding | COLING 2024 |
| ðŸ”— LINKED | [IOP](https://iopscience.iop.org/article/10.1088/1361-6552/acc299) | ChatGPT and the frustrated Socrates | Physics Education 2023 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2022.acl-long.168/) | Things not written in text: Exploring spatial commonsense from visual signals | ACL 2022 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.findings-emnlp.504/) | POSQA: Probe the World Models of LLMs with Size Comparisons | EMNLP 2023 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.acl-short.53/) | Probing physical reasoning with Counter-Commonsense context | ACL 2023 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.findings-emnlp.652/) | NEWTON: Are Large Language Models Capable of Physical Reasoning? | EMNLP 2023 |
| ðŸ“‹ TO-READ | [2502.12054](https://arxiv.org/abs/2502.12054) | PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning | arXiv |
| ðŸ“‹ TO-READ | [2502.00334](https://arxiv.org/abs/2502.00334) | UGPhysics: Undergraduate Physics Reasoning with LLMs | arXiv |
| ðŸ“‹ TO-READ | [2312.04613](https://arxiv.org/abs/2312.04613) | Testing LLM performance on the Physics GRE | arXiv |
| ðŸ“‹ TO-READ | [2504.16074](https://arxiv.org/abs/2504.16074) | PHYBench: Holistic Evaluation of Physical Perception and Reasoning | arXiv |
| ðŸ“‹ TO-READ | [2507.04766](https://arxiv.org/abs/2507.04766) | ABench-Physics: Benchmarking Physical Reasoning in LLMs | arXiv |
| ðŸ“‹ TO-READ | [2502.15815](https://arxiv.org/abs/2502.15815) | Theoretical Physics Benchmark (TPBench) | arXiv |
| ðŸ“‹ TO-READ | [2412.00821](https://arxiv.org/abs/2412.00821) | Improving Physics Reasoning Using Mixture of Refinement Agents | arXiv |
| ðŸ”— LINKED | [ACM](https://dl.acm.org/doi/10.5555/3692070.3693649) | Structured chemistry reasoning with large language models | ICML 2024 |
| ðŸ“‹ TO-READ | [2502.15224](https://arxiv.org/abs/2502.15224) | Auto-Bench: Automated Benchmark for Scientific Discovery in LLMs | arXiv |

---

## Embodied Reasoning - 2D Perception-Based Physical Reasoning

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=EIK6xxIoCB) | Core knowledge deficits in multi-modal language models | ICML 2025 |
| ðŸ”— LINKED | [ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Bitton-Guetta_Breaking_Common_Sense_WHOOPS_A_Vision-and-Language_Benchmark_of_Synthetic_and_ICCV_2023_paper.pdf) | Breaking common sense: WHOOPS! A vision-and-language benchmark | ICCV 2023 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.findings-emnlp.683.pdf) | Rome: Evaluating pre-trained VLMs on reasoning beyond visual common sense | EMNLP 2023 |
| ðŸ”— LINKED | [ACCV](https://openaccess.thecvf.com/content/ACCV2024/papers/Rahmanzadehgervi_Vision_language_models_are_blind_ACCV_2024_paper.pdf) | Vision language models are blind | ACCV 2024 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.tacl-1.37/) | Visual spatial reasoning | TACL 2023 |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/pdf/0e4134b9f62c6533e1e91fcbd5ba785cbe5e077c.pdf) | Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning? | NeurIPS 2024 |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=Q5RYn6jagC) | Understanding the limits of VLMs through the lens of the binding problem | NeurIPS 2024 |
| ðŸ”— LINKED | [2503.02199](https://arxiv.org/abs/2503.02199) | Words or Vision: Do Vision-Language Models Have Blind Faith in Text? | arXiv |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2024.findings-emnlp.763/) | Large Language Models Are Challenged by Habitat-Centered Reasoning | EMNLP 2024 |
| ðŸ”— LINKED | [Nature](https://www.nature.com/articles/s42256-024-00963-y) | Visual cognition in multimodal large language models | Nature Machine Intelligence 2025 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2023.findings-eacl.10.pdf) | Learning the effects of physical actions in a multi-modal environment | EACL 2023 |
| ðŸ“‹ TO-READ | [2412.08619](https://arxiv.org/abs/2412.08619) | Synthetic Vision: Training Vision-Language Models to Understand Physics | arXiv |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=Q6a9W6kzv5) | PhysBench: Benchmarking and Enhancing VLMs for Physical World Understanding | ICLR 2025 |
| ðŸ”— LINKED | [NeurIPS](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/d09bf41544a3365a46c9077ebb5e35c3-Paper-round1.pdf) | Physion: Evaluating physical prediction from vision in humans and machines | NeurIPS 2021 |
| ðŸ”— LINKED | [ACL](https://aclanthology.org/2022.findings-acl.205/) | Craft: A benchmark for causal reasoning about forces and interactions | ACL 2022 |
| ðŸ”— LINKED | [ACM](https://dl.acm.org/doi/10.1007/978-981-97-2262-4_5) | MM-PhyQA: Multimodal physics question-answering with multi-image CoT | PAKDD 2024 |
| ðŸ”— LINKED | [2505.15929](https://arxiv.org/abs/2505.15929) | PhyX: Does Your Model Have the "Wits" for Physical Reasoning? | arXiv |
| ðŸ“‹ TO-READ | [2411.08027](https://arxiv.org/abs/2411.08027) | Llmphy: Complex physical reasoning using LLMs and world models | arXiv |
| ðŸ”— LINKED | [AAAI](https://ojs.aaai.org/index.php/AAAI-SS/article/download/31189/33349) | Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics | AAAI 2024 |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=uBhqll8pw1) | On Inherent 3D Reasoning of VLMs in Indoor Scene Layout Design | OpenReview |
| ðŸ“‹ TO-READ | [2409.14277](https://arxiv.org/abs/2409.14277) | Can-Do! A Dataset for Embodied Planning with Large Multimodal Models | arXiv |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=fp6t3F669F) | BALROG: Benchmarking agentic LLM and VLM reasoning on games | ICLR 2025 |
| ðŸ“‹ TO-READ | [2505.05405](https://arxiv.org/abs/2505.05405) | DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning | arXiv |

---

## Embodied Reasoning - 3D Real-World Physical Reasoning

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=bdHkMjBJG_w) | Do as I can, not as I say: Grounding language in robotic affordances | CoRL 2022 |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=iSwK1YqO7v) | Embodied agent interface: Benchmarking LLMs for embodied decision making | NeurIPS 2024 |
| ðŸ”— LINKED | [IEEE](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416558) | Deploying and evaluating LLMs to program service mobile robots | IEEE 2024 |
| ðŸ”— LINKED | [PMLR](https://proceedings.mlr.press/v162/huang22a/huang22a.pdf) | Language models as zero-shot planners: Extracting actionable knowledge | ICML 2022 |
| ðŸ”— LINKED | [IEEE](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10412086) | RobotGPT: Robot manipulation learning from ChatGPT | IEEE 2024 |
| ðŸ“‹ TO-READ | [2502.14669](https://arxiv.org/abs/2502.14669) | AlphaMaze: Enhancing LLMs' Spatial Intelligence via GRPO | arXiv |
| ðŸ“‹ TO-READ | [2410.23242](https://arxiv.org/abs/2410.23242) | A little less conversation: Physical common-sense in a 3D embodied environment | arXiv |
| ðŸ“‹ TO-READ | [2310.13065](https://arxiv.org/abs/2310.13065) | Creative robot tool use with large language models | arXiv |
| ðŸ”— LINKED | [CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf) | SpatialVLM: Endowing vision-language models with spatial reasoning | CVPR 2024 |
| ðŸ“‹ TO-READ | [2410.15863](https://arxiv.org/abs/2410.15863) | Task-oriented robotic manipulation with vision language models | arXiv |
| ðŸ”— LINKED | [IEEE](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160591) | Code as policies: Language model programs for embodied control | ICRA 2023 |
| ðŸ”— LINKED | [OpenReview](https://openreview.net/forum?id=ei3qCntB66) | BadRobot: Manipulating embodied LLMs in the physical world | ICLR 2025 |

---

## General/Case-by-Case Reasoning Failure Studies

| Status | ID/Link | Title | Venue |
|--------|---------|-------|-------|
| ðŸ“‹ TO-READ | [2405.19616](https://arxiv.org/abs/2405.19616) | Easy Problems That LLMs Get Wrong | arXiv |
| ðŸ”— LINKED | [Semantic Scholar](https://www.semanticscholar.org/paper/Reasoning-with-Transformer-based-Models%3A-Deep-but-Helwe-Clavel/8424082e3bf4792462eb112d7ebcecf5b0dc3613) | Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning | AKBC 2022 |
| ðŸ“‹ TO-READ | [2302.03494](https://arxiv.org/abs/2302.03494) | A Categorical Archive of ChatGPT Failures | arXiv |

---

## Summary Statistics

| Status | Count |
|--------|-------|
| âœ… DONE (Already Analyzed) | ~20 |
| ðŸ“‹ TO-READ (Issues #48-54) | ~120 |
| ðŸ”— LINKED (Now with links!) | ~87 |
| **Total from Survey** | ~227 |

---

## Tracking Issues

| Issue | Category | Status |
|-------|----------|--------|
| [#47](https://github.com/Proteusiq/unthinking/issues/47) | Survey Paper Analysis | CLOSED |
| [#48](https://github.com/Proteusiq/unthinking/issues/48) | Cognitive Skills & Biases | OPEN |
| [#49](https://github.com/Proteusiq/unthinking/issues/49) | Theory of Mind & Social | OPEN |
| [#50](https://github.com/Proteusiq/unthinking/issues/50) | Logic & Compositional | OPEN |
| [#51](https://github.com/Proteusiq/unthinking/issues/51) | Benchmark Robustness | OPEN |
| [#52](https://github.com/Proteusiq/unthinking/issues/52) | Arithmetic & Math | OPEN |
| [#53](https://github.com/Proteusiq/unthinking/issues/53) | Embodied & Physical | OPEN |
| [#54](https://github.com/Proteusiq/unthinking/issues/54) | General Failure Studies | OPEN |
| [#55](https://github.com/Proteusiq/unthinking/issues/55) | MISSING Papers | OPEN â†’ RESOLVED |
