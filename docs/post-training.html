<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>LLM Post-Training Pipeline | The Thinking Machine That Doesn't Think</title>
<style>
* { box-sizing: border-box; margin: 0; padding: 0; }
body { background: #0d1117; color: #c9d1d9; font-family: 'Courier New', monospace; padding: 1.5rem 1rem 3rem; }

.topbar { display: flex; align-items: center; justify-content: space-between; width: 720px; margin: 0 auto 1.6rem; flex-wrap: wrap; gap: 0.5rem; }
h2 { color: #58a6ff; letter-spacing: 2px; font-size: 0.8rem; text-transform: uppercase; }
.toggle-wrap { display: flex; gap: 0; border: 1px solid #30363d; border-radius: 5px; overflow: hidden; }
.toggle-btn { background: #161b22; color: #8b949e; border: none; padding: 5px 13px; font-family: 'Courier New', monospace; font-size: 0.63rem; letter-spacing: 1px; text-transform: uppercase; cursor: pointer; transition: background 0.15s, color 0.15s; border-right: 1px solid #30363d; }
.toggle-btn:last-child { border-right: none; }
.toggle-btn.active { background: #58a6ff; color: #0d1117; font-weight: bold; }

#view-pipeline, #view-overview, #view-research { display: none; }
#view-pipeline.show, #view-overview.show, #view-research.show { display: block; }

.founding-banners { display: flex; gap: 8px; width: 720px; margin: 0 auto 1.1rem; flex-wrap: wrap; }
.fb { font-size: 0.63rem; padding: 6px 10px; border-radius: 4px; line-height: 1.65; flex: 1; min-width: 200px; }
.fb b { display: block; font-size: 0.65rem; margin-bottom: 3px; }

.outer { display: flex; gap: 0; width: 720px; margin: 0 auto; }
.timeline { width: 66px; flex-shrink: 0; display: flex; flex-direction: column; align-items: center; padding-top: 20px; }
.tl-item { display: flex; flex-direction: column; align-items: center; }
.tl-year { font-size: 0.55rem; color: #58a6ff; font-weight: bold; white-space: nowrap; }
.tl-dot { width: 8px; height: 8px; border-radius: 50%; background: #30363d; margin: 3px 0; flex-shrink: 0; }
.tl-dot.active { background: #58a6ff; box-shadow: 0 0 5px #58a6ff88; }
.tl-line { width: 2px; background: #21262d; flex: 1; min-height: 10px; }

.pipeline { flex: 1; display: flex; flex-direction: column; align-items: flex-start; }
.stage-wrap { width: 100%; }
.box { border: 1px solid #30363d; border-radius: 6px; padding: 0.75rem 0.95rem; background: #161b22; cursor: pointer; transition: border-color 0.2s, background 0.2s; width: 100%; }
.box:hover { border-color: #58a6ff; background: #1c2128; }
.box-header { display: flex; justify-content: space-between; align-items: flex-start; }
.snum { font-size: 0.55rem; letter-spacing: 2px; text-transform: uppercase; opacity: 0.55; margin-bottom: 2px; }
.sname { font-size: 0.86rem; font-weight: bold; }
.chevron { font-size: 0.6rem; color: #30363d; transition: transform 0.2s; flex-shrink: 0; padding-left: 6px; margin-top: 2px; }
.box.open .chevron { transform: rotate(180deg); color: #58a6ff; }
.sdesc-short { font-size: 0.69rem; color: #8b949e; line-height: 1.7; margin-top: 0.32rem; }
.sdesc-detail { font-size: 0.69rem; color: #8b949e; line-height: 1.8; margin-top: 0.6rem; padding-top: 0.6rem; border-top: 1px solid #21262d; display: none; }
.box.open .sdesc-detail { display: block; }
.sdesc-short b, .sdesc-detail b { color: #e6edf3; }
.math-block { background: #0d1117; border: 1px solid #21262d; border-radius: 4px; padding: 5px 9px; margin: 5px 0; font-size: 0.67rem; color: #79c0ff; line-height: 1.55; }
.crit-note { border-top: 1px dashed #f0883e; padding-top: 0.5rem; margin-top: 0.35rem; font-size: 0.67rem; }
.crit-label { color: #f0883e; font-size: 0.6rem; letter-spacing: 1px; text-transform: uppercase; font-weight: bold; margin-bottom: 0.2rem; }

.arrow { text-align: left; padding-left: 12px; color: #30363d; font-size: 1rem; line-height: 1.35; }
.arrow-label { font-size: 0.55rem; color: #30363d; letter-spacing: 1px; display: block; }

.fork3 { display: flex; gap: 7px; width: 100%; }
.fork-col { flex: 1; display: flex; flex-direction: column; }
.fork-col-label { font-size: 0.55rem; color: #58a6ff; text-align: center; letter-spacing: 1px; text-transform: uppercase; margin-bottom: 4px; }
.fork-label-row { display: flex; justify-content: center; font-size: 0.58rem; color: #ffa657; letter-spacing: 1.5px; text-transform: uppercase; margin-bottom: 5px; }
.fork-or { display: flex; align-items: center; justify-content: center; color: #21262d; font-size: 0.7rem; padding: 0 2px; flex-shrink: 0; }

.branch-split { width: 100%; display: flex; gap: 10px; }
.branch { flex: 1; display: flex; flex-direction: column; }
.branch-label { font-size: 0.57rem; text-align: center; letter-spacing: 1.5px; text-transform: uppercase; margin-bottom: 5px; padding: 3px 6px; border-radius: 3px; }
.branch-std .branch-label { color: #56d364; background: #1b2e1f; }
.branch-llm .branch-label { color: #d2a8ff; background: #2b1d3d; }
.branch-arrow { text-align: center; color: #30363d; font-size: 0.9rem; line-height: 1.3; }

.loop-wrap { width: 100%; display: flex; align-items: stretch; }
.loop-content { flex: 1; }
.loop-bar { width: 20px; display: flex; flex-direction: column; align-items: center; margin-left: 5px; }
.loop-line { flex: 1; border-left: 2px dashed #21262d; }
.loop-tag { writing-mode: vertical-rl; font-size: 0.49rem; color: #3fb950; letter-spacing: 2px; text-transform: uppercase; transform: rotate(180deg); padding: 4px 0; white-space: nowrap; }
.loop-arrow-up { color: #3fb950; font-size: 0.72rem; }

.spec-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 6px; margin-top: 6px; }
.spec-item { background: #0d1117; border-radius: 4px; padding: 5px 7px; font-size: 0.65rem; color: #8b949e; line-height: 1.6; border: 1px solid #21262d; }
.spec-item b { color: #c9d1d9; }

.labs-section-title { width: 100%; text-align: center; font-size: 0.57rem; color: #8b949e; letter-spacing: 1.5px; text-transform: uppercase; margin: 1.2rem 0 0.45rem; }
.labs { display: grid; grid-template-columns: 1fr 1fr; gap: 7px; width: 100%; }
.lab { background: #161b22; border: 1px solid #30363d; border-radius: 5px; padding: 7px 9px; font-size: 0.65rem; color: #8b949e; line-height: 1.65; }
.lab b { color: #e6edf3; }
.lab .lname { font-size: 0.58rem; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 3px; font-weight: bold; }

.c-pre{--ac:#8b949e}.c-sft{--ac:#56d364}.c-rs{--ac:#e3b341}.c-rl{--ac:#ffa657}.c-dpo{--ac:#79c0ff}.c-rlaif{--ac:#d2a8ff}.c-iter{--ac:#3fb950}.c-spec{--ac:#f0883e}.c-lrm{--ac:#d2a8ff}.c-out{--ac:#f85149}
[class*="c-"] .box{border-left:3px solid var(--ac)}
[class*="c-"] .sname{color:var(--ac)}
[class*="c-"] .snum{color:var(--ac)}

.note { width: 100%; font-size: 0.68rem; color: #8b949e; border: 1px solid #21262d; border-radius: 6px; padding: 0.82rem 1rem; background: #0d1117; line-height: 1.85; margin-top: 1.3rem; }
.note b { color: #c9d1d9; }
.note-title { font-size: 0.59rem; letter-spacing: 1.5px; text-transform: uppercase; margin-bottom: 0.42rem; }

/* OVERVIEW */
.ov-wrap { width: 720px; margin: 0 auto; display: flex; flex-direction: column; gap: 6px; }
.ov-box { border-radius: 5px; padding: 7px 11px; border: 1px solid #30363d; background: #161b22; }
.ov-name { font-size: 0.76rem; font-weight: bold; }
.ov-desc { font-size: 0.64rem; color: #8b949e; margin-top: 2px; }
.ov-arrow { color: #30363d; font-size: 0.85rem; text-align: center; width: 100%; padding-left: 18px; line-height: 1.2; }
.ov-fork { display: flex; gap: 6px; width: 100%; }
.ov-fork-item { flex: 1; border-radius: 5px; padding: 6px 9px; border: 1px solid #30363d; background: #161b22; }
.ov-fork-item .ov-name { font-size: 0.7rem; }

/* RESEARCH */
.rw { width: 720px; margin: 0 auto; display: flex; flex-direction: column; gap: 1.3rem; }
.rw-section { border: 1px solid #30363d; border-radius: 6px; background: #161b22; overflow: hidden; }
.rw-header { padding: 0.65rem 1rem; display: flex; align-items: baseline; gap: 0.7rem; border-bottom: 1px solid #21262d; background: #0d1117; cursor: pointer; }
.rw-header:hover { background: #161b22; }
.rw-num { font-size: 0.6rem; color: #58a6ff; letter-spacing: 2px; text-transform: uppercase; font-weight: bold; flex-shrink: 0; }
.rw-title { font-size: 0.88rem; font-weight: bold; flex: 1; }
.rw-chev { font-size: 0.6rem; color: #30363d; transition: transform 0.2s; }
.rw-section.open .rw-chev { transform: rotate(180deg); color: #58a6ff; }
.rw-body { font-size: 0.69rem; color: #8b949e; line-height: 1.85; padding: 0; max-height: 0; overflow: hidden; transition: max-height 0.3s ease, padding 0.3s; }
.rw-section.open .rw-body { padding: 0.82rem 1rem; max-height: 4000px; }
.rw-body b { color: #e6edf3; }
.rw-body h4 { font-size: 0.63rem; color: #58a6ff; letter-spacing: 1px; text-transform: uppercase; margin: 0.65rem 0 0.28rem; }
.rw-body h4:first-child { margin-top: 0; }
.rw-body .math-block { font-size: 0.67rem; }
.rw-body .ref { font-size: 0.63rem; color: #3fb950; margin: 2px 0; }
.rw-body .ref::before { content: "→ "; }
.rw-table { width: 100%; border-collapse: collapse; font-size: 0.67rem; margin-top: 0.5rem; }
.rw-table th { background: #0d1117; color: #58a6ff; padding: 5px 9px; text-align: left; font-size: 0.6rem; letter-spacing: 1px; text-transform: uppercase; border-bottom: 1px solid #30363d; }
.rw-table td { padding: 5px 9px; border-bottom: 1px solid #21262d; color: #8b949e; vertical-align: top; }
.rw-table td b { color: #e6edf3; }
.rw-table tr:last-child td { border-bottom: none; }
.rw-table tr:hover td { background: #1c2128; }
.open-problems { display: grid; grid-template-columns: 1fr 1fr; gap: 7px; margin-top: 0.5rem; }
.op-item { background: #0d1117; border: 1px solid #21262d; border-radius: 4px; padding: 7px 9px; font-size: 0.67rem; color: #8b949e; line-height: 1.6; }
.op-item b { color: #f0883e; }
.open-labs-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 9px; margin-top: 0.6rem; }
.ol-card { background: #0d1117; border: 1px solid #30363d; border-radius: 5px; padding: 8px 10px; font-size: 0.67rem; color: #8b949e; line-height: 1.7; }
.ol-card b { color: #e6edf3; }
.ol-name { font-size: 0.65rem; text-transform: uppercase; letter-spacing: 1px; font-weight: bold; margin-bottom: 4px; }
.ol-tag { display: inline-block; font-size: 0.56rem; border-radius: 3px; padding: 1px 5px; margin: 1px 2px; font-weight: bold; }
.common-themes { display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 7px; margin-top: 0.6rem; }
.ct-item { background: #0d1117; border: 1px solid #21262d; border-radius: 4px; padding: 6px 8px; font-size: 0.66rem; color: #8b949e; line-height: 1.6; }
.ct-item b { color: #c9d1d9; }

/* Back link */
.back-link { display: inline-flex; align-items: center; gap: 6px; color: #58a6ff; text-decoration: none; font-size: 0.7rem; margin-bottom: 1rem; opacity: 0.8; transition: opacity 0.15s; }
.back-link:hover { opacity: 1; }
.back-link svg { width: 14px; height: 14px; }
</style>
</head>
<body>

<div style="width: 720px; margin: 0 auto;">
  <a href="index.html" class="back-link">
    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
      <path d="M19 12H5M12 19l-7-7 7-7"/>
    </svg>
    Back to Paper Network
  </a>
</div>

<div class="topbar">
  <h2>LLM Post-Training</h2>
  <div class="toggle-wrap">
    <button class="toggle-btn active" id="btn-pipeline" onclick="setMode('pipeline')">Pipeline</button>
    <button class="toggle-btn" id="btn-overview" onclick="setMode('overview')">Overview</button>
    <button class="toggle-btn" id="btn-research" onclick="setMode('research')">Research</button>
  </div>
</div>

<!-- ═══════════ PIPELINE ═══════════ -->
<div id="view-pipeline" class="show">
<div class="founding-banners">
  <div class="fb" style="background:#1b2e1f22;border:1px solid #56d36433;">
    <b style="color:#56d364">★ OpenAI InstructGPT (2022) — Founding Paper</b>
    First large-scale RLHF post-training on GPT-3. Established canonical <b>SFT → RM → PPO</b> pipeline. 1.3B InstructGPT outperforms 175B GPT-3 on human preference — alignment over scale. Defined post-training as an independent field.
  </div>
  <div class="fb" style="background:#2b1d3d22;border:1px solid #d2a8ff33;">
    <b style="color:#d2a8ff">★ Anthropic Constitutional AI (2022) — Founding Paper</b>
    Self-critique + revision via written constitution — AI-generated preference labels at scale. Foundational to all RLAIF. Evolved into Claude. 2026 extension: constitutions explain <i>why</i>, enabling generalization to novel cases.
  </div>
</div>

<div class="outer">
  <div class="timeline">
    <div class="tl-item"><span class="tl-year">2022</span><div class="tl-dot active"></div></div>
    <div class="tl-item"><div class="tl-line" style="min-height:34px"></div></div>
    <div class="tl-item"><div class="tl-dot"></div></div>
    <div class="tl-item"><div class="tl-line" style="min-height:34px"></div></div>
    <div class="tl-item"><span class="tl-year">2023</span><div class="tl-dot active"></div></div>
    <div class="tl-item"><div class="tl-line" style="min-height:40px"></div></div>
    <div class="tl-item"><div class="tl-dot"></div></div>
    <div class="tl-item"><div class="tl-line" style="min-height:54px"></div></div>
    <div class="tl-item"><span class="tl-year">2024</span><div class="tl-dot active"></div></div>
    <div class="tl-item"><div class="tl-line" style="min-height:54px"></div></div>
    <div class="tl-item"><div class="tl-dot"></div></div>
    <div class="tl-item"><div class="tl-line" style="min-height:54px"></div></div>
    <div class="tl-item"><span class="tl-year">2025+</span><div class="tl-dot active"></div></div>
    <div class="tl-item"><div class="tl-line" style="flex:1"></div></div>
  </div>

  <div class="pipeline">
    <div class="stage-wrap c-pre">
      <div class="box" onclick="toggle(this)">
        <div class="box-header"><div><div class="snum">Foundation</div><div class="sname">Pretrained Base Model</div></div><span class="chevron">▼</span></div>
        <div class="sdesc-short">Next-token prediction on massive unlabeled corpora. Rich world knowledge, weak behavior.</div>
        <div class="sdesc-detail">No alignment or instruction-following. Post-training begins here. A strong base makes post-training significantly easier — most gains unlock capabilities already latent in the base.</div>
      </div>
    </div>
    <div class="arrow">│<br><span class="arrow-label">── POST-TRAINING BEGINS ──</span>▼</div>

    <div class="stage-wrap c-sft">
      <div class="box" onclick="toggle(this)">
        <div class="box-header"><div><div class="snum">Stage 1 · Est. 2022 (InstructGPT)</div><div class="sname">Supervised Fine-Tuning (SFT)</div></div><span class="chevron">▼</span></div>
        <div class="sdesc-short">Distribution shift toward instruction-following. Teaches format and prior — not world knowledge.</div>
        <div class="sdesc-detail">
          <div class="math-block">p_θ(y|x) → p_θ(y|x, instruction prior)</div>
          · Data increasingly <b>synthetic</b> (persona-driven, task-specific, decontaminated)<br>
          · <b>Tülu 3:</b> 939k prompts (57% public, 43% synthetic); <b>Llama 4:</b> prunes &gt;50% "easy" data<br>
          · <b>DeepSeek-V3:</b> 1.5M instances — reasoning distilled from R1 + human-verified<br>
          · PEFT: <b>LoRA / DoRA</b> at r=8–16 implies low intrinsic dimension of behavioral updates
          <div class="crit-note"><div class="crit-label">⚠ Critical</div>LIMA "style steering" hypothesis holds for generic instruction-following. Breaks for reasoning scaffolds (cold-start CoT SFT is structurally necessary for RLVR stability) and genuine domain gaps.</div>
        </div>
      </div>
    </div>
    <div class="arrow">│<br>▼</div>

    <div style="width:100%;text-align:center;font-size:0.57rem;color:#8b949e;letter-spacing:1.5px;text-transform:uppercase;margin-bottom:6px;">Pipeline diverges here</div>
    <div class="branch-split">

      <!-- STANDARD -->
      <div class="branch branch-std">
        <div class="branch-label">Standard Alignment</div>
        <div class="stage-wrap c-rs">
          <div class="box" onclick="toggle(this)">
            <div class="box-header"><div><div class="snum">Stage 2 · 2023+</div><div class="sname">Rejection Sampling</div></div><span class="chevron">▼</span></div>
            <div class="sdesc-short">Best-of-N filtering via RM or verifier. Improves data quality without full RL.</div>
            <div class="sdesc-detail">
              · <b>Tülu 3:</b> explicit stage; on-policy generations vs other models<br>
              · <b>DeepSeek-V3:</b> RS on R1 rollouts — concise, formatted, verified<br>
              · <b>Kimi k1.5:</b> shortest rejection sampling for long2short transfer
              <div class="crit-note"><div class="crit-label">⚠ Critical</div>Selecting only correct outputs biases toward "easy" correct responses.</div>
            </div>
          </div>
        </div>
        <div class="branch-arrow">│<br>▼</div>
        <div style="width:100%">
          <div class="fork-label-row">Stage 3 · Preference Alignment</div>
          <div class="fork3">
            <div class="fork-col c-rl">
              <div class="fork-col-label">Path A</div>
              <div class="box" onclick="toggle(this)">
                <div class="box-header"><div><div class="snum">2022–present</div><div class="sname">RLHF / GRPO</div></div><span class="chevron">▼</span></div>
                <div class="sdesc-short">Reward model + PPO/GRPO. KL-regularized RL.</div>
                <div class="sdesc-detail">
                  <div class="math-block">max_π E[r(x,y)] − β·D_KL(π ‖ π_SFT)</div>
                  <b>GRPO:</b> no value fn, group-normalize rewards:<br>
                  <div class="math-block">A_i = (r_i − mean(r)) / std(r)</div>
                  <b>CISPO</b> (MiniMax M1): clips IS weights; all tokens in gradient — more efficient than GRPO/DAPO.
                </div>
              </div>
            </div>
            <div class="fork-or">or</div>
            <div class="fork-col c-dpo">
              <div class="fork-col-label">Path B</div>
              <div class="box" onclick="toggle(this)">
                <div class="box-header"><div><div class="snum">2023–dominant</div><div class="sname">DPO / IPO</div></div><span class="chevron">▼</span></div>
                <div class="sdesc-short">Contrastive log-likelihood. No RM needed.</div>
                <div class="sdesc-detail">
                  <div class="math-block">log σ(β(log π_θ(y_w|x) − log π_θ(y_l|x)))</div>
                  <b>Tülu 3:</b> length-normalized DPO<br>
                  <b>Llama 3.1/4, OpenAI:</b> primary preference method
                  <div class="crit-note"><div class="crit-label">⚠ Critical</div>Static offline data — no exploration. Ceiling lower than online RL for reasoning.</div>
                </div>
              </div>
            </div>
            <div class="fork-or">or</div>
            <div class="fork-col c-rlaif">
              <div class="fork-col-label">Path C</div>
              <div class="box" onclick="toggle(this)">
                <div class="box-header"><div><div class="snum">2022–present</div><div class="sname">RLAIF / CAI</div></div><span class="chevron">▼</span></div>
                <div class="sdesc-short">AI judge + constitution. Self-distillation with normative constraints.</div>
                <div class="sdesc-detail">
                  Pioneered by <b>Anthropic Constitutional AI (2022).</b><br><br>
                  <b>Claude 2026:</b> constitution explains <i>why</i> for novel case generalization.<br>
                  Open question: converges to human alignment or recursive self-consistency?
                  <div class="crit-note"><div class="crit-label">⚠ Critical</div>Amplifies existing model biases at scale. Constitution encodes authors' assumptions.</div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="branch-arrow">│<br>▼</div>
        <div class="loop-wrap">
          <div class="loop-content c-iter">
            <div class="box" onclick="toggle(this)">
              <div class="box-header"><div><div class="snum">Stage 4 · 2024+</div><div class="sname">Iterative Refinement &amp; Synthetic Loops</div></div><span class="chevron">▼</span></div>
              <div class="sdesc-short">Model-generated synthetic data each round. SFT and RL under unified objective.</div>
              <div class="sdesc-detail">
                <b>Tülu 3:</b> eval-driven iteration — benchmark suite identifies skill gaps<br>
                <b>DeepSeek-V3:</b> R1 distillation → V3 SFT → V3 alignment<br>
                <b>Kimi:</b> curriculum + prioritized sampling (hard/underperformed problems)
                <div class="math-block">SFT = offline exploitation (low variance, high bias)<br>RL  = online exploration  (high variance, low bias)</div>
                <div class="crit-note"><div class="crit-label">⚠ Critical</div>No convergence guarantee. Systematic errors entrench rather than self-correct.</div>
              </div>
            </div>
          </div>
          <div class="loop-bar"><div class="loop-tag">iterative loop</div><div class="loop-line"></div><div class="loop-arrow-up">↑</div></div>
        </div>
      </div>

      <!-- REASONING -->
      <div class="branch branch-llm">
        <div class="branch-label">Reasoning (LRM)</div>
        <div class="stage-wrap c-sft">
          <div class="box" onclick="toggle(this)">
            <div class="box-header"><div><div class="snum">Step 1</div><div class="sname">SFT on Long CoT</div></div><span class="chevron">▼</span></div>
            <div class="sdesc-short">Cold-start: curated long CoT traces. Teaches &lt;think&gt; scaffold.</div>
            <div class="sdesc-detail"><b>R1-Zero skips this entirely</b> — direct GRPO on verifiable problems. Full R1 uses limited human-aligned CoT cold-start first.</div>
          </div>
        </div>
        <div class="branch-arrow">│<br>▼</div>
        <div class="stage-wrap c-lrm">
          <div class="box" onclick="toggle(this)">
            <div class="box-header"><div><div class="snum">Step 2 · 2024–2025</div><div class="sname">RLVR / Pure RL</div></div><span class="chevron">▼</span></div>
            <div class="sdesc-short">Verifiable outcome rewards. No RM. GRPO/CISPO. Emergent self-reflection.</div>
            <div class="sdesc-detail">
              <b>DeepSeek-R1-Zero:</b> 80k verifiable problems, 16 samples/group. Emergent: self-reflection, strategy switching, 5–7× longer CoT.<br><br>
              <b>MiniMax M1 (CISPO):</b> clips IS weights, all tokens in gradient. 512 H800s, 3 weeks, ~$534k. AIME 68%→80%.<br><br>
              <b>OLMo 3 RL Zero:</b> per-domain checkpoints (math/code/IFeval) — open resource for contamination research in RLVR.<br><br>
              <b>Tülu 3 RLVR:</b> deterministic verifiers. +1.7 MATH, +3.3 GSM8K from DPO baseline.
            </div>
          </div>
        </div>
        <div class="branch-arrow">│<br>▼</div>
        <div class="stage-wrap c-rs">
          <div class="box" onclick="toggle(this)">
            <div class="box-header"><div><div class="snum">Step 3</div><div class="sname">RS + SFT (stabilize)</div></div><span class="chevron">▼</span></div>
            <div class="sdesc-short">Filter best RLVR rollouts, mix with general data, crystallize gains.</div>
            <div class="sdesc-detail">· Keep: correct + readable + well-formatted<br>· Mix with general instruction data (prevents regression)<br>· ~2 epochs — distills RL-discovered reasoning into stable behavior</div>
          </div>
        </div>
        <div class="branch-arrow">│<br>▼</div>
        <div class="stage-wrap c-rl">
          <div class="box" onclick="toggle(this)">
            <div class="box-header"><div><div class="snum">Step 4</div><div class="sname">RLHF / RFT Polish</div></div><span class="chevron">▼</span></div>
            <div class="sdesc-short">Final alignment pass. Process supervision on CoT steps (OpenAI RFT).</div>
            <div class="sdesc-detail"><b>R1 full:</b> second RL stage with preference RM + rule rewards (language consistency)<br><b>OpenAI RFT:</b> expert grader scores reasoning <i>process</i> step-by-step.</div>
          </div>
        </div>
      </div>
    </div>

    <div class="arrow">│<br>▼</div>
    <div class="stage-wrap c-spec">
      <div class="box" onclick="toggle(this)">
        <div class="box-header"><div><div class="snum">Stage 5 · 2024–2026</div><div class="sname">Specialized Enhancements</div></div><span class="chevron">▼</span></div>
        <div class="sdesc-short">Reasoning depth, factuality, multimodal, agentic, safety, efficiency.</div>
        <div class="sdesc-detail"><div class="spec-grid">
          <div class="spec-item"><b>Reasoning (LRM)</b><br>Process supervision, exploratory RL, self-verification</div>
          <div class="spec-item"><b>Factuality</b><br>FLAME: factuality-aware SFT + DPO (Meta)</div>
          <div class="spec-item"><b>Agentic / long-context</b><br>Tool-use synthesis, joint environment RL (Kimi K2, GLM-4.5, MiniMax M2.5)</div>
          <div class="spec-item"><b>Multimodal</b><br>Zero-vision SFT + joint text-vision RL (Kimi K2.5, Llama 4)</div>
          <div class="spec-item"><b>Safety</b><br>Red-team preference passes · targeted DPO</div>
          <div class="spec-item"><b>Efficiency</b><br>Quantization · distillation · PEFT · linear attention (MiniMax)</div>
        </div></div>
      </div>
    </div>
    <div class="arrow">│<br>▼</div>
    <div class="stage-wrap c-out">
      <div class="box" style="cursor:default">
        <div class="snum">Output</div>
        <div class="sname">Production-Ready Aligned Model</div>
        <div class="sdesc-short">Helpful · Honest · Harmless · Instruction-following · Reasoning-capable · Agentic</div>
      </div>
    </div>

    <!-- MAJOR LABS -->
    <div class="labs-section-title">Major Labs</div>
    <div class="labs">
      <div class="lab"><div class="lname" style="color:#ffa657">Meta (Llama 3.1 / 4)</div><b>3.1:</b> Iterative SFT + RS + DPO.<br><b>4:</b> SFT → Online RL (hard prompts) → DPO polish. Prunes easy data via model-as-judge.</div>
      <div class="lab"><div class="lname" style="color:#d2a8ff">Anthropic (Claude)</div>Constitution-driven synthetic data + RLAIF. Originated 2022. 2026: constitution explains <i>why</i>, enabling generalization across novel cases.</div>
      <div class="lab"><div class="lname" style="color:#79c0ff">OpenAI</div>SFT → DPO for standard models. RFT for reasoning: expert grader scores CoT process step-by-step.</div>
      <div class="lab"><div class="lname" style="color:#56d364">Google DeepMind</div>Instruction tuning + RLHF. PPO-based pipeline. Gemini 1.5/2 adds long-context alignment passes.</div>
      <div class="lab"><div class="lname" style="color:#58a6ff">Microsoft (Phi-3 / Phi-4)</div>"Textbook-quality" synthetic SFT. <b>Orca / Orca-2:</b> step-level reasoning strategies via process supervision. Frontier reasoning at 3B–14B.</div>
      <div class="lab"><div class="lname" style="color:#c9d1d9">xAI (Grok-3)</div><b>DeepThink:</b> GRPO-style reasoning RL. Standard SFT + DPO for base.</div>
      <div class="lab"><div class="lname" style="color:#76b900">NVIDIA (Nemotron-4)</div><b>HelpSteer2</b> synthetic preference pipeline + open reward model adopted by other labs.</div>
      <div class="lab"><div class="lname" style="color:#c9d1d9">MiniMax (M1 / M2.5)</div><b>M1:</b> CISPO on hybrid MoE + Lightning Attn base (456B/45.9B active, 1M ctx).<br><b>M2.5:</b> 80.2% SWE-Bench, 76.3% BrowseComp. RL across 100k+ real-world environments.</div>
    </div>

    <div class="labs-section-title">Open-weights labs</div>
    <div class="labs">
      <div class="lab"><div class="lname" style="color:#e3b341">AllenAI (Tülu 3 / OLMo 3) <span style="color:#56d364;font-size:0.58rem">★ fully open</span></div>Weights + data + code + eval + decontamination scripts.<br><b>Tülu 3:</b> SFT → DPO → RLVR. 70B matches GPT-4o-mini.<br><b>OLMo 3 (Nov 2025):</b> Dolci suite (Think-SFT → Think-DPO → RLVR). RL Zero: direct GRPO, no cold-start. OLMoTrace: links reasoning to training data. Best fully open 32B thinking model at release. OLMo 3.1 (Dec 2025): +5 AIME, +20 IFBench.</div>
      <div class="lab"><div class="lname" style="color:#f0883e">DeepSeek (V3 / R1) <span style="color:#79c0ff;font-size:0.58rem">open weights</span></div><b>V3:</b> SFT 1.5M (R1-distilled) + GRPO. <b>R1-Zero:</b> pure GRPO, no SFT — emergent long-CoT. 79.8% AIME 2024, 97.3% MATH-500.</div>
      <div class="lab"><div class="lname" style="color:#f85149">Moonshot AI (Kimi k1.5 / K2 / K2.5)</div><b>k1.5:</b> SFT → online mirror-descent RL (length penalty, curriculum). <b>K2.5:</b> zero-vision SFT + joint text-vision RL.</div>
      <div class="lab"><div class="lname" style="color:#c9d1d9">Zhipu AI (GLM-4.5 / GLM-5)</div><b>Slime</b> async RL framework (Megatron + SGLang decoupled). APRIL for agentic RL. 744B/40B active. Huawei Ascend.</div>
      <div class="lab"><div class="lname" style="color:#79c0ff">Alibaba / Qwen</div>GRPO for math. QwQ-32B: RLVR extended CoT. SFT → GRPO → DPO.</div>
      <div class="lab"><div class="lname" style="color:#56d364">Shanghai AI Lab (InternLM)</div><b>COOL RLHF:</b> down-weights compromised preference pairs — mitigates Goodhart's Law.</div>
      <div class="lab"><div class="lname" style="color:#ffa657">ByteDance (Seed / Doubao)</div><b>Seed-Thinking v1.5:</b> GRPO on verifiable problems. Doubao deployment scale makes efficiency-constrained post-training practically significant.</div>
    </div>

    <div class="note" style="border-color:#58a6ff33;">
      <div class="note-title" style="color:#58a6ff">Trends 2024–2026</div>
      <b>Verifiable / rule-based RL surge</b> — RLVR, GRPO, CISPO: emergent long-CoT without human trajectories.<br>
      <b>Synthetic + on-policy dominance</b> — distillation loops, persona generation, agentic synthesis.<br>
      <b>DPO over PPO</b> — stable and scalable; PPO/GRPO reserved for reasoning or online exploration.<br>
      <b>Linear attention at frontier scale</b> — MiniMax M1/M2.5: O(n) Lightning Attention now competitive.<br>
      <b>Agentic focus</b> — long-context RL, tool-use, joint multimodal RL (Kimi K2, GLM-4.5, MiniMax M2.5).<br>
      <b>Full openness raises the floor</b> — Tülu 3, OLMo 3, Nemotron-4 as fully open blueprints; DeepSeek-R1, MiniMax-M1 as open weights.
    </div>
  </div>
</div>
</div>

<!-- ═══════════ OVERVIEW ═══════════ -->
<div id="view-overview">
<div class="ov-wrap">
  <div class="ov-box" style="border-left:3px solid #8b949e"><div class="ov-name" style="color:#8b949e">Pretrained Base Model</div><div class="ov-desc">Next-token prediction · no alignment · rich latent knowledge</div></div>
  <div class="ov-arrow">│<br>▼</div>
  <div class="ov-box" style="border-left:3px solid #56d364"><div class="ov-name" style="color:#56d364">1 · SFT</div><div class="ov-desc">Instruction prior · persona-driven synthetic data · format compliance</div></div>
  <div class="ov-arrow">│<br>▼</div>
  <div style="font-size:0.57rem;color:#8b949e;letter-spacing:1.5px;text-transform:uppercase;text-align:center;margin-bottom:5px;">Pipeline diverges</div>
  <div style="display:flex;gap:8px;width:100%">
    <div style="flex:1;display:flex;flex-direction:column;gap:5px">
      <div style="font-size:0.57rem;color:#56d364;text-align:center;letter-spacing:1px;text-transform:uppercase;background:#1b2e1f;border-radius:3px;padding:2px">Standard Alignment</div>
      <div class="ov-box" style="border-left:3px solid #e3b341"><div class="ov-name" style="color:#e3b341">2 · Rejection Sampling</div><div class="ov-desc">Best-of-N · on-policy data quality</div></div>
      <div style="text-align:center;color:#30363d;font-size:0.8rem">▼</div>
      <div class="ov-fork">
        <div class="ov-fork-item" style="border-left:3px solid #ffa657"><div class="ov-name" style="color:#ffa657">RLHF/GRPO</div><div class="ov-desc">RM + KL-reg RL</div></div>
        <div class="ov-fork-item" style="border-left:3px solid #79c0ff"><div class="ov-name" style="color:#79c0ff">DPO</div><div class="ov-desc">Contrastive log-ratio</div></div>
        <div class="ov-fork-item" style="border-left:3px solid #d2a8ff"><div class="ov-name" style="color:#d2a8ff">RLAIF/CAI</div><div class="ov-desc">AI judge + constitution</div></div>
      </div>
      <div style="text-align:center;color:#30363d;font-size:0.8rem">▼</div>
      <div class="ov-box" style="border-left:3px solid #3fb950"><div class="ov-name" style="color:#3fb950">4 · Iterative Loops</div><div class="ov-desc">Synthetic escalation · skill-gap eval · distillation</div></div>
    </div>
    <div style="flex:1;display:flex;flex-direction:column;gap:5px">
      <div style="font-size:0.57rem;color:#d2a8ff;text-align:center;letter-spacing:1px;text-transform:uppercase;background:#2b1d3d;border-radius:3px;padding:2px">Reasoning / LRM</div>
      <div class="ov-box" style="border-left:3px solid #56d364"><div class="ov-name" style="color:#56d364">SFT on Long CoT</div><div class="ov-desc">Cold-start · &lt;think&gt; scaffold (or skip for R1-Zero)</div></div>
      <div style="text-align:center;color:#30363d;font-size:0.8rem">▼</div>
      <div class="ov-box" style="border-left:3px solid #d2a8ff"><div class="ov-name" style="color:#d2a8ff">RLVR / CISPO / Pure RL</div><div class="ov-desc">Verifiable rewards · GRPO/CISPO · emergent self-reflection</div></div>
      <div style="text-align:center;color:#30363d;font-size:0.8rem">▼</div>
      <div class="ov-box" style="border-left:3px solid #e3b341"><div class="ov-name" style="color:#e3b341">RS + SFT</div><div class="ov-desc">Crystallize RL gains · mix general data</div></div>
      <div style="text-align:center;color:#30363d;font-size:0.8rem">▼</div>
      <div class="ov-box" style="border-left:3px solid #ffa657"><div class="ov-name" style="color:#ffa657">RLHF / RFT Polish</div><div class="ov-desc">Process supervision on CoT steps</div></div>
    </div>
  </div>
  <div class="ov-arrow">│<br>▼</div>
  <div class="ov-box" style="border-left:3px solid #f0883e"><div class="ov-name" style="color:#f0883e">5 · Specialized Enhancements</div><div class="ov-desc">Reasoning · factuality · agentic · multimodal · safety · efficiency</div></div>
  <div class="ov-arrow">│<br>▼</div>
  <div class="ov-box" style="border-left:3px solid #f85149"><div class="ov-name" style="color:#f85149">Aligned Model</div><div class="ov-desc">Helpful · Honest · Harmless · Reasoning-capable · Agentic</div></div>
  <div style="margin-top:1.2rem;font-size:0.57rem;color:#8b949e;letter-spacing:1.5px;text-transform:uppercase;text-align:center;margin-bottom:5px;">Lab quick-reference</div>
  <div style="display:grid;grid-template-columns:1fr 1fr 1fr 1fr;gap:6px;width:100%">
    <div class="ov-box" style="border-left:3px solid #e3b341;padding:6px 8px"><div class="ov-name" style="color:#e3b341;font-size:0.68rem">Tülu 3 / OLMo 3</div><div class="ov-desc">SFT→DPO→RLVR<br>Fully open recipe</div></div>
    <div class="ov-box" style="border-left:3px solid #f0883e;padding:6px 8px"><div class="ov-name" style="color:#f0883e;font-size:0.68rem">DeepSeek R1</div><div class="ov-desc">Pure GRPO<br>No SFT cold-start</div></div>
    <div class="ov-box" style="border-left:3px solid #f85149;padding:6px 8px"><div class="ov-name" style="color:#f85149;font-size:0.68rem">Kimi k1.5</div><div class="ov-desc">Mirror-descent RL<br>Length penalty</div></div>
    <div class="ov-box" style="border-left:3px solid #c9d1d9;padding:6px 8px"><div class="ov-name" style="color:#c9d1d9;font-size:0.68rem">MiniMax M1/M2.5</div><div class="ov-desc">CISPO + Lightning<br>Attn · 1M ctx</div></div>
  </div>
</div>
</div>

<!-- ═══════════ RESEARCH ═══════════ -->
<div id="view-research">
<div class="rw">

  <div class="rw-section open">
    <div class="rw-header" onclick="toggleSection(this.parentElement)"><span class="rw-num">§1</span><span class="rw-title" style="color:#56d364">Supervised Fine-Tuning (SFT)</span><span class="rw-chev">▼</span></div>
    <div class="rw-body">
      <h4>Core function</h4>
      <div class="math-block">p_θ(y|x)  →  p_θ(y|x, instruction prior)</div>
      <h4>Canonical references</h4>
      <div class="ref">Ouyang et al. (2022) — InstructGPT. Established SFT as post-training stage 1.</div>
      <div class="ref">Zhou et al. (2023) — LIMA. SFT data quality > quantity; surface-form hypothesis.</div>
      <div class="ref">Hu et al. (2021) — LoRA. Low-rank adaptation implies low intrinsic dimension of behavioral SFT.</div>
      <h4>Steering vs. rewriting</h4>
      LoRA's effectiveness at r=8–16 implies weight updates are low-rank:
      <div class="math-block">ΔW = BA,  rank(B,A) ≪ rank(W)</div>
      <b>SFT is a spectrum:</b> style steering for generic instruction-following; representational surgery for genuine domain gaps; structurally necessary cold-start for reasoning RL.
    </div>
  </div>

  <div class="rw-section">
    <div class="rw-header" onclick="toggleSection(this.parentElement)"><span class="rw-num">§2</span><span class="rw-title" style="color:#ffa657">Preference Optimization</span><span class="rw-chev">▼</span></div>
    <div class="rw-body">
      <h4>2.1 — RLHF (PPO)</h4>
      <div class="ref">Ouyang et al. (2022) — InstructGPT; Bai et al. (2022) — Constitutional AI</div>
      <div class="math-block">max_π  E[r(x,y)]  −  β · D_KL(π ‖ π_SFT)</div>
      Conservative by construction. Expensive but only regime with genuine online exploration.
      <h4>2.2 — DPO</h4>
      <div class="ref">Rafailov et al. (2023) — DPO: Direct Preference Optimization</div>
      <div class="math-block">ℒ_DPO = −log σ(β(log π_θ(y_w|x) − log π_θ(y_l|x)))</div>
      Dominant in Tülu 3, Llama 3.1/4, OpenAI. Static offline data; capability ceiling on complex reasoning.
      <h4>2.3 — GRPO</h4>
      Removes value function. Group-normalized advantages:
      <div class="math-block">A_i = (r_i − mean(r)) / std(r),  i ∈ group G</div>
      Core of DeepSeek-R1, V3, Tülu RLVR, Kimi k1.5.
      <h4>2.4 — CISPO (MiniMax M1, 2025)</h4>
      <div class="ref">MiniMax (2025) — MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention</div>
      Clips importance sampling weights; <b>all tokens contribute to gradient</b> — GRPO/DAPO discard clipped tokens, CISPO uses them. 512 H800s, 3 weeks, ~$534k.
      <div class="math-block">CISPO: clip(IS weights) — no token waste — vs GRPO: discard clipped tokens</div>
      <h4>2.5 — RLVR (Verifiable Rewards)</h4>
      <div class="ref">Lambert et al. (2024) — Tülu 3; DeepSeek-AI (2025) — R1; Team OLMo (2025) — OLMo 3</div>
      Deterministic verifiers: answer matching (math), compiler checks (code), constraint verification. Eliminates RM hacking. Enables emergent long-CoT. OLMo 3 RL Zero releases per-domain checkpoints to study RLVR contamination.
    </div>
  </div>

  <div class="rw-section">
    <div class="rw-header" onclick="toggleSection(this.parentElement)"><span class="rw-num">§3</span><span class="rw-title" style="color:#d2a8ff">Self-Alignment / AI Feedback (RLAIF / CAI)</span><span class="rw-chev">▼</span></div>
    <div class="rw-body">
      <div class="ref">Bai et al. (2022) — Constitutional AI (Anthropic) — founding paper of RLAIF</div>
      <div class="ref">Lee et al. (2023) — RLAIF: Scaling Reinforcement Learning from Human Feedback</div>
      <b>Constitutional AI (2022):</b> self-critique + revision guided by a written constitution — replacing human preference labels with AI-generated ones at scale. Foundational to all RLAIF work.<br><br>
      <b>2026 extension:</b> constitutions explain <i>why</i> a principle holds — generalization to novel edge cases rather than rule-matching.<br><br>
      <b>Open question:</b> converges to human-aligned behavior or recursive self-consistency fixed points? Constitution provides normative anchor but encodes authors' assumptions. RLAIF at scale is annotator bias amplification, not elimination.
    </div>
  </div>

  <div class="rw-section">
    <div class="rw-header" onclick="toggleSection(this.parentElement)"><span class="rw-num">§4</span><span class="rw-title" style="color:#3fb950">Test-Time Optimization &amp; Reasoning Scaling</span><span class="rw-chev">▼</span></div>
    <div class="rw-body">
      <div class="ref">Snell et al. (2024) — Scaling LLM Test-Time Compute Optimally</div>
      <div class="ref">OpenAI (2024) — o1; DeepSeek-AI (2025) — R1; MiniMax (2025) — M1</div>
      Post-training is no longer a discrete phase — it becomes <b>continuous inference-time optimization</b>.<br><br>
      <b>MiniMax M1:</b> Lightning Attention makes test-time compute scaling economically viable — ~25% FLOPs of DeepSeek-R1 at 100K generation length. First linear-attention model competitive at frontier scale.<br><br>
      <b>Implication:</b> the boundary between training-time alignment and inference-time reasoning is dissolving.
    </div>
  </div>

  <div class="rw-section">
    <div class="rw-header" onclick="toggleSection(this.parentElement)"><span class="rw-num">§5</span><span class="rw-title" style="color:#f0883e">Representation vs. Behavior</span><span class="rw-chev">▼</span></div>
    <div class="rw-body">
      Does post-training change <b>cognition</b> (internal representations) or only <b>surface policy</b> (output distribution)?<br><br>
      Current mechanistic evidence: core world models largely preserved through SFT and DPO. Alignment layers act as <b>policy heads</b>. Supports "alignment as wrapper" hypothesis.<br><br>
      <b>Where this breaks:</b> RLVR on reasoning appears to genuinely alter internal computational pathways — emergent self-verification and extended CoT are not obvious pretraining artifacts. Wrapper hypothesis holds for alignment SFT/DPO; under-evidenced for capability-expanding RL.
    </div>
  </div>

  <div class="rw-section">
    <div class="rw-header" onclick="toggleSection(this.parentElement)"><span class="rw-num">§6</span><span class="rw-title" style="color:#58a6ff">Open-Weights Labs: Recipes &amp; Innovations</span><span class="rw-chev">▼</span></div>
    <div class="rw-body">
      <div class="open-labs-grid">
        <div class="ol-card">
          <div class="ol-name" style="color:#58a6ff">Microsoft — Phi-3 / Phi-4</div>
          <span class="ol-tag" style="background:#0d2233;color:#58a6ff">2024</span>
          <span class="ol-tag" style="background:#1b2e1f;color:#56d364">open weights</span><br><br>
          "Textbook-quality" synthetic data — quality filtering baked into generation. Frontier reasoning at 3B–14B. <b>Orca/Orca-2:</b> process supervision for step-level reasoning strategies — trains <i>how</i> to reason, not just what to answer.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#c9d1d9">MiniMax — M1 / M2.5</div>
          <span class="ol-tag" style="background:#21262d;color:#c9d1d9">M1 Jun 2025</span>
          <span class="ol-tag" style="background:#21262d;color:#c9d1d9">M2.5 Feb 2026</span>
          <span class="ol-tag" style="background:#1b2e1f;color:#56d364">open weights</span><br><br>
          <b>Architecture:</b> Hybrid MoE + Lightning Attention O(n). 456B/45.9B active. Native 1M ctx. &lt;50% FLOPs vs R1 at 64K tokens.<br><br>
          <b>CISPO:</b> clips IS weights; all tokens in gradient (vs GRPO/DAPO which discard clipped). 512 H800s, 3 weeks, ~$534k.<br><br>
          <b>M2.5:</b> 80.2% SWE-Bench, 51.3% Multi-SWE-Bench, 76.3% BrowseComp. RL across 100k+ real-world environments. $1/hr at 100 tok/s.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#e3b341">AllenAI — Tülu 3 / OLMo 3</div>
          <span class="ol-tag" style="background:#2d2200;color:#e3b341">Tülu 3 Nov 2024</span>
          <span class="ol-tag" style="background:#2d2200;color:#e3b341">OLMo 3 Nov 2025</span>
          <span class="ol-tag" style="background:#1b2e1f;color:#56d364">★ fully open</span><br><br>
          <b>Tülu 3:</b> Persona-driven SFT 939k → length-normalized DPO → RLVR (deterministic verifiers, no RM). 70B matches GPT-4o-mini.<br><br>
          <b>OLMo 3:</b> Dolci suite — Think-SFT → Think-DPO → RLVR. <b>RL Zero:</b> direct GRPO, no cold-start SFT; per-domain checkpoints released to study RLVR contamination. <b>OLMoTrace:</b> links reasoning traces to training documents — first interpretability tool for thinking models. 65k ctx, Apache 2.0.<br><br>
          <b>OLMo 3.1 (Dec 2025):</b> +21-day RL extension, +5 AIME, +4 IFEval, +20 IFBench.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#c9d1d9">xAI — Grok-3</div>
          <span class="ol-tag" style="background:#21262d;color:#c9d1d9">Feb 2025</span><br><br>
          Standard SFT + DPO. <b>DeepThink:</b> GRPO-style RL reasoning — mirrors R1. Verifiable rewards for math/coding; emergent self-correction under extended inference budget.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#79c0ff">Alibaba — Qwen2.5 / QwQ</div>
          <span class="ol-tag" style="background:#1f2d3d;color:#79c0ff">2024–2025</span>
          <span class="ol-tag" style="background:#1b2e1f;color:#56d364">open weights</span><br><br>
          <b>Qwen2.5-Math:</b> GRPO, binary outcome rewards, no RM. <b>QwQ-32B:</b> RLVR extended CoT + self-verification. SFT → GRPO → DPO. Independent validation of GRPO+RLVR outside DeepSeek.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#56d364">Shanghai AI Lab — InternLM</div>
          <span class="ol-tag" style="background:#1b2e1f;color:#56d364">open weights</span><br><br>
          <b>COOL RLHF:</b> identifies "compromised" preference pairs (RM likely exploited), down-weights them. Directly addresses Goodhart's Law. SFT → COOL RLHF → DPO. <b>OpenCompass</b> eval framework widely adopted for decontaminated benchmarking.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#76b900">NVIDIA — Nemotron-4</div>
          <span class="ol-tag" style="background:#1a2200;color:#76b900">2024</span>
          <span class="ol-tag" style="background:#1b2e1f;color:#56d364">★ fully open</span><br><br>
          <b>HelpSteer2</b> synthetic (chosen, rejected) preference pairs. <b>Nemotron-4 340B Reward</b> model widely adopted by other labs as off-the-shelf RM. Demonstrates synthetic preference generation + open RMs can substitute for human annotation.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#ffa657">ByteDance — Seed / Doubao</div>
          <span class="ol-tag" style="background:#2d1f0e;color:#ffa657">2025</span><br><br>
          <b>Seed-Thinking v1.5:</b> GRPO on verifiable problems — emergent long-CoT, mirrors R1-Zero. <b>Seed-Coder:</b> on-policy RL with execution-based reward. Doubao deployment scale makes efficiency-constrained post-training practically significant.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#f0883e">DeepSeek — V3 / R1</div>
          <span class="ol-tag" style="background:#2d1500;color:#f0883e">Dec 2024 / Jan 2025</span>
          <span class="ol-tag" style="background:#1b2e1f;color:#56d364">open weights</span><br><br>
          <b>V3:</b> SFT 1.5M (R1-distilled) + GRPO (rule-based + model RM). <b>R1-Zero:</b> no SFT, direct GRPO on 80k verifiable problems. Emergent: self-reflection, strategy switching, 5–7× longer CoT.<br>
          79.8% AIME 2024, 97.3% MATH-500, Codeforces 96th percentile.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#f85149">Moonshot AI — Kimi k1.5 / K2 / K2.5</div>
          <span class="ol-tag" style="background:#2d0f0e;color:#f85149">2025–2026</span><br><br>
          <b>k1.5:</b> SFT → long-CoT warmup → online mirror-descent RL (relative entropy reg., no value net). Length penalty + curriculum. <b>K2:</b> agentic data synthesis + joint RL. <b>K2.5:</b> zero-vision SFT + joint text-vision RL. Agent Swarm orchestration.
        </div>
        <div class="ol-card">
          <div class="ol-name" style="color:#c9d1d9">Zhipu AI / Z.ai — GLM-4.5 / GLM-5</div>
          <span class="ol-tag" style="background:#21262d;color:#c9d1d9">GLM-5 Feb 2026</span>
          <span class="ol-tag" style="background:#1b2e1f;color:#56d364">MIT license</span><br><br>
          <b>Slime:</b> async RL — decouples Megatron training from SGLang inference, eliminates synchronous long-tail bottleneck. <b>APRIL:</b> Active Partial Rollouts for multi-step agentic RL. 744B/40B active. 77.8% SWE-bench. Trained on Huawei Ascend — zero NVIDIA dependency.
        </div>
      </div>
      <h4>Common themes</h4>
      <div class="common-themes">
        <div class="ct-item"><b>Synthetic-heavy SFT</b><br>Persona-driven, task-specific, decontaminated. Majority synthetic in 2024+.</div>
        <div class="ct-item"><b>GRPO / CISPO / DPO over PPO</b><br>No value network. PPO retained only where online RL adds unique value.</div>
        <div class="ct-item"><b>Verifiable RL surge</b><br>Rule-based RMs, outcome-only rewards — emergent CoT without human trajectories.</div>
        <div class="ct-item"><b>Distillation loops</b><br>R1→V3, on-policy Tülu, agentic Kimi. Model as its own improving teacher.</div>
        <div class="ct-item"><b>Eval-centric iteration</b><br>Skill-gap analysis, decontamination, dev/unseen splits drive data mix.</div>
        <div class="ct-item"><b>Agentic / multimodal RL</b><br>Tool-use synthesis, joint environment RL, vision integration now standard.</div>
      </div>
    </div>
  </div>

  <div class="rw-section">
    <div class="rw-header" onclick="toggleSection(this.parentElement)"><span class="rw-num">§7</span><span class="rw-title" style="color:#f85149">Open Research Problems</span><span class="rw-chev">▼</span></div>
    <div class="rw-body">
      <div class="open-problems">
        <div class="op-item"><b>Alignment tax</b><br>Does preference optimization degrade reasoning? Evidence mixed; confounded by benchmark saturation.</div>
        <div class="op-item"><b>Reward misspecification</b><br>Reward models inherit annotator biases. Goodhart's Law at scale.</div>
        <div class="op-item"><b>DPO distributional collapse</b><br>Static preferences can collapse output diversity.</div>
        <div class="op-item"><b>Iterative loop convergence</b><br>No convergence guarantee for self-generating data loops.</div>
        <div class="op-item"><b>Internal goal formation</b><br>Does RL post-training induce internal goal representations or only behavioral tendencies?</div>
        <div class="op-item"><b>Post-training × scaling laws</b><br>Current scaling laws inadequately model the post-training regime.</div>
        <div class="op-item"><b>RLVR contamination</b><br>Do benchmark gains reflect genuine reasoning or data contamination? OLMo 3 RL Zero checkpoints address this directly.</div>
        <div class="op-item"><b>RLAIF convergence</b><br>Does CAI converge to human-aligned behavior or self-consistency fixed points?</div>
      </div>
    </div>
  </div>

  <div class="rw-section open">
    <div class="rw-header" onclick="toggleSection(this.parentElement)"><span class="rw-num">§8</span><span class="rw-title" style="color:#58a6ff">Structural Summary</span><span class="rw-chev">▼</span></div>
    <div class="rw-body">
      <table class="rw-table">
        <thead><tr><th>Method</th><th>Mathematical form</th><th>Function</th><th>Key limitation</th></tr></thead>
        <tbody>
          <tr><td><b style="color:#56d364">SFT</b></td><td>MLE on curated / synthetic data</td><td>Instruction prior · format</td><td>Ceiling bounded by demo quality</td></tr>
          <tr><td><b style="color:#ffa657">RLHF (PPO)</b></td><td>max E[r] − β·D_KL(π ‖ π_SFT)</td><td>Preference shaping · online exploration</td><td>Expensive · RM misspecification</td></tr>
          <tr><td><b style="color:#79c0ff">DPO</b></td><td>Contrastive log-likelihood on (y_w, y_l)</td><td>Efficient preference optimization</td><td>Static data · no exploration</td></tr>
          <tr><td><b style="color:#ffa657">GRPO</b></td><td>Group-normalized advantage + KL</td><td>RL without value function</td><td>Discards clipped tokens</td></tr>
          <tr><td><b style="color:#c9d1d9">CISPO</b> <span style="color:#8b949e;font-size:0.6rem">(MiniMax)</span></td><td>Clip IS weights; all tokens in gradient</td><td>Efficient RL, no token waste</td><td>Needs reliable reward signal</td></tr>
          <tr><td><b style="color:#e3b341">RLVR</b></td><td>Binary verifier reward ∈ {0,1}</td><td>Emergent reasoning · no RM hacking</td><td>Limited to verifiable domains</td></tr>
          <tr><td><b style="color:#d2a8ff">RLAIF / CAI</b></td><td>Self-distillation + normative constraints</td><td>Scalable alignment</td><td>Amplifies model biases at scale</td></tr>
          <tr><td><b style="color:#3fb950">Test-time opt.</b></td><td>Search / process supervision</td><td>Inference-time reasoning scaling</td><td>Expensive · blurs training boundary</td></tr>
          <tr><td><b style="color:#f85149">Mirror-descent RL</b> <span style="color:#8b949e;font-size:0.6rem">(Kimi)</span></td><td>Relative entropy reg. + group sampling</td><td>Long-context reasoning · conciseness</td><td>No value network limits credit assignment</td></tr>
        </tbody>
      </table>
    </div>
  </div>

</div>
</div>

<div style="max-width:720px;margin:2rem auto 0;padding-top:1rem;border-top:1px solid #21262d;font-size:0.58rem;color:#8b949e;line-height:1.8">
  <span style="color:#30363d;text-transform:uppercase;letter-spacing:1.5px;font-size:0.5rem">Sources</span><br>
  Synthesized from primary research papers cited above
</div>

<script>
function toggle(box){
  if(!document.getElementById('view-pipeline').classList.contains('show')) return;
  box.classList.toggle('open');
}
function toggleSection(sec){ sec.classList.toggle('open'); }
function setMode(m){
  ['pipeline','overview','research'].forEach(v=>{
    document.getElementById('view-'+v).classList.toggle('show',v===m);
    document.getElementById('btn-'+v).classList.toggle('active',v===m);
  });
}
</script>
</body>
</html>
